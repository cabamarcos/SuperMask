{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynQ_78kDeHs2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from ast import Param\n",
        "import json\n",
        "\n",
        "from utils.prune import prune_weights\n",
        "from utils.count_improvement import improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC2dq3fNeHs3",
        "outputId": "f25ae62f-1fb0-4f04-ba95-416d32f19c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Verificar si la GPU está disponible y establecer el dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PpJbsf7NeHs3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qzJaokBeHs3"
      },
      "source": [
        "Definimos las redes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8L94OKmYeHs4"
      },
      "outputs": [],
      "source": [
        "net = Net().to(device)\n",
        "varianzas_net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8W9t_3beHs4"
      },
      "source": [
        "Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO_Ykr29eHs4",
        "outputId": "16ad4e8d-dace-485a-dcae-f338879a2c3e"
      },
      "outputs": [],
      "source": [
        "# Definimos el transform para los datos de MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Cargamos el dataset de MNIST\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Definimos los DataLoaders para los conjuntos de entrenamiento y prueba\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cWAORhpoeHs4"
      },
      "outputs": [],
      "source": [
        "# Definimos la función de pérdida para calcular el error\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HTF5fm-peHs4",
        "outputId": "cee40ff8-e105-42b7-a23d-9b5948930430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Train loss: 2.29719615084276\n",
            "Test accuracy: 0.1318\n",
            "Epoch 2\n",
            "Train loss: 2.3180963128868703\n",
            "Test accuracy: 0.0822\n",
            "Epoch 3\n",
            "Train loss: 2.307574129307956\n",
            "Test accuracy: 0.1313\n",
            "Epoch 4\n",
            "Train loss: 2.2910919240288643\n",
            "Test accuracy: 0.1083\n",
            "Epoch 5\n",
            "Train loss: 2.3179781475046806\n",
            "Test accuracy: 0.1238\n",
            "Epoch 6\n",
            "Train loss: 2.3169236833861135\n",
            "Test accuracy: 0.0823\n",
            "Epoch 7\n",
            "Train loss: 2.316894697736321\n",
            "Test accuracy: 0.129\n",
            "Epoch 8\n",
            "Train loss: 2.3114416680610512\n",
            "Test accuracy: 0.1059\n",
            "Epoch 9\n",
            "Train loss: 2.323011710191332\n",
            "Test accuracy: 0.0966\n",
            "Epoch 10\n",
            "Train loss: 2.280967342573951\n",
            "Test accuracy: 0.1215\n",
            "Epoch 11\n",
            "Train loss: 2.3155754687689516\n",
            "Test accuracy: 0.0912\n",
            "Epoch 12\n",
            "Train loss: 2.3323536216577234\n",
            "Test accuracy: 0.1262\n",
            "Epoch 13\n",
            "Train loss: 2.427624556555677\n",
            "Test accuracy: 0.0802\n",
            "Epoch 14\n",
            "Train loss: 2.6269157159048864\n",
            "Test accuracy: 0.0719\n",
            "Epoch 15\n",
            "Train loss: 3.2924850787689435\n",
            "Test accuracy: 0.0925\n",
            "Epoch 16\n",
            "Train loss: 4.636529459882138\n",
            "Test accuracy: 0.15\n",
            "Epoch 17\n",
            "Train loss: 4.780121603754283\n",
            "Test accuracy: 0.1153\n",
            "Epoch 18\n",
            "Train loss: 3.142615231877959\n",
            "Test accuracy: 0.1104\n",
            "Epoch 19\n",
            "Train loss: 3.154797956379238\n",
            "Test accuracy: 0.1122\n",
            "Epoch 20\n",
            "Train loss: 3.182076690547756\n",
            "Test accuracy: 0.0885\n",
            "Epoch 21\n",
            "Train loss: 2.4830583923661123\n",
            "Test accuracy: 0.0838\n",
            "Epoch 22\n",
            "Train loss: 2.3963041940985965\n",
            "Test accuracy: 0.085\n",
            "Epoch 23\n",
            "Train loss: 2.532875456281308\n",
            "Test accuracy: 0.1495\n",
            "Epoch 24\n",
            "Train loss: 4.156155110168051\n",
            "Test accuracy: 0.0508\n",
            "Epoch 25\n",
            "Train loss: 7.409466336785095\n",
            "Test accuracy: 0.1149\n",
            "Epoch 26\n",
            "Train loss: 8.475702119026103\n",
            "Test accuracy: 0.0508\n",
            "Epoch 27\n",
            "Train loss: 14.508708640710632\n",
            "Test accuracy: 0.1179\n",
            "Epoch 28\n",
            "Train loss: 15.467411913597253\n",
            "Test accuracy: 0.1325\n",
            "Epoch 29\n",
            "Train loss: 12.782150950767338\n",
            "Test accuracy: 0.0876\n",
            "Epoch 30\n",
            "Train loss: 31.763523494256837\n",
            "Test accuracy: 0.0699\n",
            "Epoch 31\n",
            "Train loss: 25.14331218987894\n",
            "Test accuracy: 0.092\n",
            "Epoch 32\n",
            "Train loss: 40.83786642729346\n",
            "Test accuracy: 0.0757\n",
            "Epoch 33\n",
            "Train loss: 28.81031840594847\n",
            "Test accuracy: 0.0889\n",
            "Epoch 34\n",
            "Train loss: 60.615928141547165\n",
            "Test accuracy: 0.1003\n",
            "Epoch 35\n",
            "Train loss: 67.8755799153212\n",
            "Test accuracy: 0.0928\n",
            "Epoch 36\n",
            "Train loss: 126.30063176917623\n",
            "Test accuracy: 0.1322\n",
            "Epoch 37\n",
            "Train loss: 331.84773091720876\n",
            "Test accuracy: 0.1774\n",
            "Epoch 38\n",
            "Train loss: 481.93926279682086\n",
            "Test accuracy: 0.142\n",
            "Epoch 39\n",
            "Train loss: 413.42995877459106\n",
            "Test accuracy: 0.1059\n",
            "Epoch 40\n",
            "Train loss: 1017.0437841354403\n",
            "Test accuracy: 0.1021\n",
            "Epoch 41\n",
            "Train loss: 1087.383943081919\n",
            "Test accuracy: 0.112\n",
            "Epoch 42\n",
            "Train loss: 1037.8289970609426\n",
            "Test accuracy: 0.1104\n",
            "Epoch 43\n",
            "Train loss: 710.7024695552997\n",
            "Test accuracy: 0.0881\n",
            "Epoch 44\n",
            "Train loss: 1647.85301159694\n",
            "Test accuracy: 0.0617\n",
            "Epoch 45\n",
            "Train loss: 2830.956314835213\n",
            "Test accuracy: 0.0796\n",
            "Epoch 46\n",
            "Train loss: 7587.771223056036\n",
            "Test accuracy: 0.0881\n",
            "Epoch 47\n",
            "Train loss: 11039.777623288413\n",
            "Test accuracy: 0.0916\n",
            "Epoch 48\n",
            "Train loss: 19105.31332247801\n",
            "Test accuracy: 0.0941\n",
            "Epoch 49\n",
            "Train loss: 11509.60418818297\n",
            "Test accuracy: 0.1566\n",
            "Epoch 50\n",
            "Train loss: 28166.03930612007\n",
            "Test accuracy: 0.1062\n",
            "Epoch 51\n",
            "Train loss: 62016.99872567964\n",
            "Test accuracy: 0.1097\n",
            "Epoch 52\n",
            "Train loss: 54073.814457289445\n",
            "Test accuracy: 0.1157\n",
            "Epoch 53\n",
            "Train loss: 60548.24910464419\n",
            "Test accuracy: 0.0533\n",
            "Epoch 54\n",
            "Train loss: 59763.99294126466\n",
            "Test accuracy: 0.1368\n",
            "Epoch 55\n",
            "Train loss: 108858.94786946962\n",
            "Test accuracy: 0.0504\n",
            "Epoch 56\n",
            "Train loss: 224892.35077958423\n",
            "Test accuracy: 0.1286\n",
            "Epoch 57\n",
            "Train loss: 407999.37813166314\n",
            "Test accuracy: 0.0706\n",
            "Epoch 58\n",
            "Train loss: 654217.2949426973\n",
            "Test accuracy: 0.095\n",
            "Epoch 59\n",
            "Train loss: 640508.8236273987\n",
            "Test accuracy: 0.0888\n",
            "Epoch 60\n",
            "Train loss: 1217363.0283182303\n",
            "Test accuracy: 0.0882\n",
            "Epoch 61\n",
            "Train loss: 1979782.0766257995\n",
            "Test accuracy: 0.0961\n",
            "Epoch 62\n",
            "Train loss: 2551851.49586887\n",
            "Test accuracy: 0.0393\n",
            "Epoch 63\n",
            "Train loss: 2001348.190031983\n",
            "Test accuracy: 0.0705\n",
            "Epoch 64\n",
            "Train loss: 2564862.9049840085\n",
            "Test accuracy: 0.0813\n",
            "Epoch 65\n",
            "Train loss: 1790518.498933902\n",
            "Test accuracy: 0.1203\n",
            "Epoch 66\n",
            "Train loss: 2619993.856876333\n",
            "Test accuracy: 0.1404\n",
            "Epoch 67\n",
            "Train loss: 8041155.51705757\n",
            "Test accuracy: 0.0984\n",
            "Epoch 68\n",
            "Train loss: 11573969.496801706\n",
            "Test accuracy: 0.1287\n",
            "Epoch 69\n",
            "Train loss: 12050465.424307037\n",
            "Test accuracy: 0.12\n",
            "Epoch 70\n",
            "Train loss: 15280726.421108741\n",
            "Test accuracy: 0.1007\n",
            "Epoch 71\n",
            "Train loss: 10164560.018123668\n",
            "Test accuracy: 0.0776\n",
            "Epoch 72\n",
            "Train loss: 22187102.86673774\n",
            "Test accuracy: 0.126\n",
            "Epoch 73\n",
            "Train loss: 19200755.918976545\n",
            "Test accuracy: 0.0767\n",
            "Epoch 74\n",
            "Train loss: 41068845.50959488\n",
            "Test accuracy: 0.0526\n",
            "Epoch 75\n",
            "Train loss: 34431193.28358209\n",
            "Test accuracy: 0.1238\n",
            "Epoch 76\n",
            "Train loss: 64946977.14285714\n",
            "Test accuracy: 0.0947\n",
            "Epoch 77\n",
            "Train loss: 124751245.65458423\n",
            "Test accuracy: 0.0579\n",
            "Epoch 78\n",
            "Train loss: 297122710.4648188\n",
            "Test accuracy: 0.057\n",
            "Epoch 79\n",
            "Train loss: 723983051.7356076\n",
            "Test accuracy: 0.1028\n",
            "Epoch 80\n",
            "Pesos guardados en la época 80 en 'pesos_epoca_80.txt'.\n",
            "Train loss: 634296165.7995735\n",
            "Test accuracy: 0.0804\n",
            "Epoch 81\n",
            "Train loss: 1549749308.8614073\n",
            "Test accuracy: 0.0913\n",
            "Epoch 82\n",
            "Train loss: 2525116641.5692964\n",
            "Test accuracy: 0.113\n",
            "Epoch 83\n",
            "Train loss: 2836187099.974414\n",
            "Test accuracy: 0.055\n",
            "Epoch 84\n",
            "Train loss: 2820340587.9402986\n",
            "Test accuracy: 0.0946\n",
            "Epoch 85\n",
            "Train loss: 1984217635.4797442\n",
            "Test accuracy: 0.1218\n",
            "Epoch 86\n",
            "Train loss: 3893066907.837953\n",
            "Test accuracy: 0.0775\n",
            "Epoch 87\n",
            "Train loss: 7476117487.078891\n",
            "Test accuracy: 0.0775\n",
            "Epoch 88\n",
            "Train loss: 13881809628.520256\n",
            "Test accuracy: 0.0581\n",
            "Epoch 89\n",
            "Train loss: 25987466687.59062\n",
            "Test accuracy: 0.1311\n",
            "Epoch 90\n",
            "Train loss: 23290455851.121536\n",
            "Test accuracy: 0.0871\n",
            "Epoch 91\n",
            "Train loss: 62045004741.04904\n",
            "Test accuracy: 0.0933\n",
            "Epoch 92\n",
            "Train loss: 103954477309.27078\n",
            "Test accuracy: 0.0996\n",
            "Epoch 93\n",
            "Train loss: 144606272758.72067\n",
            "Test accuracy: 0.0929\n",
            "Epoch 94\n",
            "Train loss: 144535799987.03625\n",
            "Test accuracy: 0.0923\n",
            "Epoch 95\n",
            "Train loss: 192933143995.22388\n",
            "Test accuracy: 0.0981\n",
            "Epoch 96\n",
            "Train loss: 207659126006.72067\n",
            "Test accuracy: 0.0956\n",
            "Epoch 97\n",
            "Train loss: 196169803046.7548\n",
            "Test accuracy: 0.093\n",
            "Epoch 98\n",
            "Train loss: 323470732220.31555\n",
            "Test accuracy: 0.0867\n",
            "Epoch 99\n",
            "Train loss: 574342211160.4264\n",
            "Test accuracy: 0.0967\n",
            "Epoch 100\n",
            "Train loss: 532599695984.4435\n",
            "Test accuracy: 0.0985\n",
            "Epoch 101\n",
            "Train loss: 1111466317059.8208\n",
            "Test accuracy: 0.0904\n",
            "Epoch 102\n",
            "Train loss: 2329654204560.1025\n",
            "Test accuracy: 0.0956\n",
            "Epoch 103\n",
            "Train loss: 3748078079013.117\n",
            "Test accuracy: 0.098\n",
            "Epoch 104\n",
            "Train loss: 4091262727026.081\n",
            "Test accuracy: 0.1309\n",
            "Epoch 105\n",
            "Train loss: 3401309745335.403\n",
            "Test accuracy: 0.1115\n",
            "Epoch 106\n",
            "Train loss: 6462979289166.602\n",
            "Test accuracy: 0.1174\n",
            "Epoch 107\n",
            "Train loss: 8364446061757.953\n",
            "Test accuracy: 0.1059\n",
            "Epoch 108\n",
            "Train loss: 5239901630822.072\n",
            "Test accuracy: 0.1112\n",
            "Epoch 109\n",
            "Train loss: 10833719706892.555\n",
            "Test accuracy: 0.1166\n",
            "Epoch 110\n",
            "Train loss: 11696950058711.062\n",
            "Test accuracy: 0.1085\n",
            "Epoch 111\n",
            "Train loss: 12942986301710.738\n",
            "Test accuracy: 0.1434\n",
            "Epoch 112\n",
            "Train loss: 12122155298754.865\n",
            "Test accuracy: 0.0974\n",
            "Epoch 113\n",
            "Train loss: 25541262366479.83\n",
            "Test accuracy: 0.1081\n",
            "Epoch 114\n",
            "Train loss: 31559819278951.71\n",
            "Test accuracy: 0.1356\n",
            "Epoch 115\n",
            "Train loss: 24553481190969.86\n",
            "Test accuracy: 0.0762\n",
            "Epoch 116\n",
            "Train loss: 47549853994265.66\n",
            "Test accuracy: 0.1414\n",
            "Epoch 117\n",
            "Train loss: 138243616144010.64\n",
            "Test accuracy: 0.1263\n",
            "Epoch 118\n",
            "Train loss: 134193480549714.42\n",
            "Test accuracy: 0.1022\n",
            "Epoch 119\n",
            "Train loss: 159980172963534.34\n",
            "Test accuracy: 0.0886\n",
            "Epoch 120\n",
            "Train loss: 310049744037848.7\n",
            "Test accuracy: 0.0938\n",
            "Epoch 121\n",
            "Train loss: 795240017938235.5\n",
            "Test accuracy: 0.0897\n",
            "Epoch 122\n",
            "Train loss: 573518621557749.1\n",
            "Test accuracy: 0.0847\n",
            "Epoch 123\n",
            "Train loss: 1093429873668698.6\n",
            "Test accuracy: 0.1098\n",
            "Epoch 124\n",
            "Train loss: 1881257735464292.0\n",
            "Test accuracy: 0.114\n",
            "Epoch 125\n",
            "Train loss: 3245620449819030.0\n",
            "Test accuracy: 0.1192\n",
            "Epoch 126\n",
            "Train loss: 1742115928909714.8\n",
            "Test accuracy: 0.1522\n",
            "Epoch 127\n",
            "Train loss: 4528687171241613.0\n",
            "Test accuracy: 0.1033\n",
            "Epoch 128\n",
            "Train loss: 4771456316781042.0\n",
            "Test accuracy: 0.1367\n",
            "Epoch 129\n",
            "Train loss: 3576432042960891.5\n",
            "Test accuracy: 0.0624\n",
            "Epoch 130\n",
            "Train loss: 8066776503227722.0\n",
            "Test accuracy: 0.105\n",
            "Epoch 131\n",
            "Train loss: 1.1529210424147934e+16\n",
            "Test accuracy: 0.1201\n",
            "Epoch 132\n",
            "Train loss: 1.647899589019137e+16\n",
            "Test accuracy: 0.0514\n",
            "Epoch 133\n",
            "Train loss: 1.2987659409824586e+16\n",
            "Test accuracy: 0.0779\n",
            "Epoch 134\n",
            "Train loss: 3.011903174967198e+16\n",
            "Test accuracy: 0.0412\n",
            "Epoch 135\n",
            "Train loss: 4.012690614878725e+16\n",
            "Test accuracy: 0.1213\n",
            "Epoch 136\n",
            "Train loss: 3.6600227309760104e+16\n",
            "Test accuracy: 0.1137\n",
            "Epoch 137\n",
            "Train loss: 1.124183991490433e+17\n",
            "Test accuracy: 0.0635\n",
            "Epoch 138\n",
            "Train loss: 2.224716074282859e+17\n",
            "Test accuracy: 0.0979\n",
            "Epoch 139\n",
            "Train loss: 1.6060487146445014e+17\n",
            "Test accuracy: 0.1816\n",
            "Epoch 140\n",
            "Train loss: 3.08775599690158e+17\n",
            "Test accuracy: 0.1424\n",
            "Epoch 141\n",
            "Train loss: 5.3333310542269856e+17\n",
            "Test accuracy: 0.1053\n",
            "Epoch 142\n",
            "Train loss: 8.591607033583328e+17\n",
            "Test accuracy: 0.0737\n",
            "Epoch 143\n",
            "Train loss: 6.63835518759206e+17\n",
            "Test accuracy: 0.1203\n",
            "Epoch 144\n",
            "Train loss: 1.4168015787291666e+18\n",
            "Test accuracy: 0.0959\n",
            "Epoch 145\n",
            "Train loss: 2.3994736729781673e+18\n",
            "Test accuracy: 0.0785\n",
            "Epoch 146\n",
            "Train loss: 2.489848860838732e+18\n",
            "Test accuracy: 0.1143\n",
            "Epoch 147\n",
            "Train loss: 2.373286842493435e+18\n",
            "Test accuracy: 0.0827\n",
            "Epoch 148\n",
            "Train loss: 6.284911425191809e+18\n",
            "Test accuracy: 0.0596\n",
            "Epoch 149\n",
            "Train loss: 4.638339222066085e+18\n",
            "Test accuracy: 0.0882\n",
            "Epoch 150\n",
            "Train loss: 9.398849132537534e+18\n",
            "Test accuracy: 0.0769\n",
            "Epoch 151\n",
            "Train loss: 1.5787877987273966e+19\n",
            "Test accuracy: 0.1135\n",
            "Epoch 152\n",
            "Train loss: 2.9679770619126936e+19\n",
            "Test accuracy: 0.0802\n",
            "Epoch 153\n",
            "Train loss: 3.3241093936772866e+19\n",
            "Test accuracy: 0.0912\n",
            "Epoch 154\n",
            "Train loss: 3.1149425765760315e+19\n",
            "Test accuracy: 0.1121\n",
            "Epoch 155\n",
            "Train loss: 9.764680727238353e+19\n",
            "Test accuracy: 0.1019\n",
            "Epoch 156\n",
            "Train loss: 9.014040669281457e+19\n",
            "Test accuracy: 0.1292\n",
            "Epoch 157\n",
            "Train loss: 1.5456339428212552e+20\n",
            "Test accuracy: 0.0959\n",
            "Epoch 158\n",
            "Train loss: 3.2182169584167485e+20\n",
            "Test accuracy: 0.0484\n",
            "Epoch 159\n",
            "Train loss: 3.823622191059137e+20\n",
            "Test accuracy: 0.0748\n",
            "Epoch 160\n",
            "Train loss: 3.979248156894907e+20\n",
            "Test accuracy: 0.1001\n",
            "Epoch 161\n",
            "Train loss: 3.085057289946256e+20\n",
            "Test accuracy: 0.0901\n",
            "Epoch 162\n",
            "Train loss: 6.576620075080592e+20\n",
            "Test accuracy: 0.0794\n",
            "Epoch 163\n",
            "Train loss: 9.358022106699653e+20\n",
            "Test accuracy: 0.1115\n",
            "Epoch 164\n",
            "Train loss: 1.5692909775204214e+21\n",
            "Test accuracy: 0.083\n",
            "Epoch 165\n",
            "Train loss: 1.0986830389641473e+21\n",
            "Test accuracy: 0.088\n",
            "Epoch 166\n",
            "Train loss: 7.771571100248846e+20\n",
            "Test accuracy: 0.1061\n",
            "Epoch 167\n",
            "Train loss: 1.644756065990318e+21\n",
            "Test accuracy: 0.1392\n",
            "Epoch 168\n",
            "Train loss: 2.7886099234658756e+21\n",
            "Test accuracy: 0.128\n",
            "Epoch 169\n",
            "Train loss: 6.281994719268123e+21\n",
            "Test accuracy: 0.1095\n",
            "Epoch 170\n",
            "Train loss: 1.532965762054243e+22\n",
            "Test accuracy: 0.122\n",
            "Epoch 171\n",
            "Train loss: 1.1722723404842004e+22\n",
            "Test accuracy: 0.132\n",
            "Epoch 172\n",
            "Train loss: 3.1464422303685564e+22\n",
            "Test accuracy: 0.0974\n",
            "Epoch 173\n",
            "Train loss: 3.591853316389428e+22\n",
            "Test accuracy: 0.0852\n",
            "Epoch 174\n",
            "Train loss: 6.7610784456166496e+22\n",
            "Test accuracy: 0.0885\n",
            "Epoch 175\n",
            "Train loss: 7.448742007419794e+22\n",
            "Test accuracy: 0.1091\n",
            "Epoch 176\n",
            "Train loss: 5.545703721136566e+22\n",
            "Test accuracy: 0.1026\n",
            "Epoch 177\n",
            "Train loss: 7.836548899526283e+22\n",
            "Test accuracy: 0.0982\n",
            "Epoch 178\n",
            "Train loss: 3.7623255231250485e+22\n",
            "Test accuracy: 0.0953\n",
            "Epoch 179\n",
            "Train loss: 6.556143166765438e+22\n",
            "Test accuracy: 0.105\n",
            "Epoch 180\n",
            "Train loss: 1.4179551241882891e+23\n",
            "Test accuracy: 0.1007\n",
            "Epoch 181\n",
            "Train loss: 1.3667086204328277e+23\n",
            "Test accuracy: 0.1188\n",
            "Epoch 182\n",
            "Train loss: 2.6938345433354205e+23\n",
            "Test accuracy: 0.0954\n",
            "Epoch 183\n",
            "Train loss: 3.524863333735125e+23\n",
            "Test accuracy: 0.0915\n",
            "Epoch 184\n",
            "Train loss: 1.0599359058798702e+24\n",
            "Test accuracy: 0.0821\n",
            "Epoch 185\n",
            "Train loss: 1.6013839902378293e+24\n",
            "Test accuracy: 0.1199\n",
            "Epoch 186\n",
            "Train loss: 1.6038006870952697e+24\n",
            "Test accuracy: 0.0712\n",
            "Epoch 187\n",
            "Train loss: 1.4208541392620115e+24\n",
            "Test accuracy: 0.1304\n",
            "Epoch 188\n",
            "Train loss: 1.836867058934219e+24\n",
            "Test accuracy: 0.099\n",
            "Epoch 189\n",
            "Train loss: 1.8757894406460107e+24\n",
            "Test accuracy: 0.0842\n",
            "Epoch 190\n",
            "Train loss: 1.6722445464694566e+24\n",
            "Test accuracy: 0.0832\n",
            "Epoch 191\n",
            "Train loss: 1.325174655397935e+24\n",
            "Test accuracy: 0.0552\n",
            "Epoch 192\n",
            "Train loss: 2.7919747986835306e+24\n",
            "Test accuracy: 0.0832\n",
            "Epoch 193\n",
            "Train loss: 5.928291679538783e+24\n",
            "Test accuracy: 0.0864\n",
            "Epoch 194\n",
            "Train loss: 1.1624033885629248e+25\n",
            "Test accuracy: 0.0983\n",
            "Epoch 195\n",
            "Train loss: 1.3774543754715586e+25\n",
            "Test accuracy: 0.0638\n",
            "Epoch 196\n",
            "Train loss: 2.9061512225712514e+25\n",
            "Test accuracy: 0.0843\n",
            "Epoch 197\n",
            "Train loss: 2.2364620618317585e+25\n",
            "Test accuracy: 0.0743\n",
            "Epoch 198\n",
            "Train loss: 5.252941324094729e+25\n",
            "Test accuracy: 0.085\n",
            "Epoch 199\n",
            "Train loss: 8.545145685100846e+25\n",
            "Test accuracy: 0.0584\n",
            "Epoch 200\n",
            "Train loss: 1.3175937131077737e+26\n",
            "Test accuracy: 0.1079\n",
            "Epoch 201\n",
            "Train loss: 8.352248072980697e+25\n",
            "Test accuracy: 0.0871\n",
            "Epoch 202\n",
            "Train loss: 4.732175455204351e+25\n",
            "Test accuracy: 0.139\n",
            "Epoch 203\n",
            "Train loss: 9.721334034083947e+25\n",
            "Test accuracy: 0.1585\n",
            "Epoch 204\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Pasamos todas las imagenes de train por la red net\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpruned_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\_functional_tensor.py:925\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 925\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train loop\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "epoch = 1\n",
        "accuracy_threshold = 0.4\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    if epoch <= 9:\n",
        "\n",
        "        state_dict_red1 = net.state_dict()\n",
        "        state_dict_red2 = varianzas_net.state_dict()\n",
        "\n",
        "        # Crear un nuevo diccionario de estado donde sumamos los pesos\n",
        "        state_dict_suma = {}\n",
        "        for key in state_dict_red1:\n",
        "            if state_dict_red1[key].size() == state_dict_red2[key].size():  # Asegurar que las dimensiones coincidan\n",
        "                # Asegurar que las desviaciones estándar sean positivas para generar el ruido\n",
        "                std_dev = torch.abs(state_dict_red2[key])\n",
        "                \n",
        "                # Generamos los valores aleatorios con una distribución normal usando torch.normal\n",
        "                noise = torch.normal(0, std_dev)  # Media = 0, Desviación estándar = std_dev\n",
        "                \n",
        "                # Crear una máscara para determinar si debemos sumar o restar\n",
        "                mask_negativa = state_dict_red2[key] < 0  # Máscara de valores negativos\n",
        "                \n",
        "                # Aplicar la operación de suma o resta dependiendo de la máscara\n",
        "                state_dict_suma[key] = torch.where(mask_negativa, state_dict_red1[key] - noise, state_dict_red1[key] + noise)\n",
        "            else:\n",
        "                # Si los tamaños no coinciden, copiamos directamente\n",
        "                state_dict_suma[key] = state_dict_red1[key]\n",
        "\n",
        "        # Crear una nueva red o modificar una existente con los pesos sumados\n",
        "        varied_net = Net().to(device)\n",
        "        varied_net.load_state_dict(state_dict_suma)\n",
        "\n",
        "        pruned_net = prune_weights(varied_net)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        # Pasamos todas las imagenes de train por la red net\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs_net = pruned_net(images)\n",
        "            loss = criterion(outputs_net, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss.append(running_loss / len(train_loader))\n",
        "        print(f\"Train loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Evaluamos el modelo en el conjunto de test\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs_net = pruned_net(images)\n",
        "                _, predicted = torch.max(outputs_net.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracies.append(correct / total)\n",
        "        print(f\"Test accuracy: {correct / total}\")\n",
        "\n",
        "        if correct / total > accuracy_threshold:\n",
        "            break\n",
        "\n",
        "    else:\n",
        "        # Modificación: Guardar los pesos en la época 80\n",
        "        if epoch == 80:\n",
        "            # Convertir el state_dict a un formato que se pueda escribir en un archivo txt\n",
        "            state_dict_suma_serializable = {key: value.cpu().numpy().tolist() for key, value in state_dict_suma.items()}\n",
        "            # Convertir el state_dict a un formato que se pueda escribir en un archivo txt\n",
        "            state_dict_var_serializable = {key: value.cpu().numpy().tolist() for key, value in state_dict_red2.items()}\n",
        "\n",
        "            # Guardar en un archivo de texto\n",
        "            with open(\"pesos_epoca_80.txt\", \"w\") as f:\n",
        "                json.dump(state_dict_suma_serializable, f)\n",
        "\n",
        "            with open(\"varianzas_epoca_80.txt\", \"w\") as f:\n",
        "                json.dump(state_dict_var_serializable, f)\n",
        "\n",
        "            print(f\"Pesos guardados en la época 80 en 'pesos_epoca_80.txt'.\")\n",
        "\n",
        "        # Sumamos los pesos de las redes en una red nueva\n",
        "        state_dict_red1 = net.state_dict()\n",
        "        state_dict_red2 = varianzas_net.state_dict()\n",
        "\n",
        "        state_dict_suma = {}\n",
        "        for key in state_dict_red1:\n",
        "            if state_dict_red1[key].size() == state_dict_red2[key].size():  # Asegurar que las dimensiones coincidan\n",
        "                # Asegurar que las desviaciones estándar sean positivas para generar el ruido\n",
        "                std_dev = torch.abs(state_dict_red2[key])\n",
        "                \n",
        "                # Generamos los valores aleatorios con una distribución normal usando torch.normal\n",
        "                noise = torch.normal(0, std_dev)  # Media = 0, Desviación estándar = std_dev\n",
        "                \n",
        "                # Crear una máscara para determinar si debemos sumar o restar\n",
        "                mask_negativa = state_dict_red2[key] < 0  # Máscara de valores negativos\n",
        "                \n",
        "                # Aplicar la operación de suma o resta dependiendo de la máscara\n",
        "                state_dict_suma[key] = torch.where(mask_negativa, state_dict_red1[key] - noise, state_dict_red1[key] + noise)\n",
        "            else:\n",
        "                # Si los tamaños no coinciden, copiamos directamente\n",
        "                state_dict_suma[key] = state_dict_red1[key]\n",
        "\n",
        "\n",
        "        # Crear una nueva red o modificar una existente con los pesos sumados\n",
        "        varied_net = Net().to(device)\n",
        "        varied_net.load_state_dict(state_dict_suma)\n",
        "\n",
        "        #print(varied_net.state_dict())\n",
        "\n",
        "        pruned_net = prune_weights(varied_net)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        # Pasamos todas las imagenes de train por la red net\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs_net = pruned_net(images)\n",
        "            loss = criterion(outputs_net, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss.append(running_loss / len(train_loader))\n",
        "        print(f\"Train loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Evaluamos el modelo en el conjunto de test\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs_net = pruned_net(images)\n",
        "                _, predicted = torch.max(outputs_net.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracies.append(correct / total)\n",
        "        print(f\"Test accuracy: {correct / total}\")\n",
        "\n",
        "        if correct / total > accuracy_threshold:\n",
        "            break\n",
        "\n",
        "        # actualizmos el vector de varianzas\n",
        "        if improvements(train_loss) > 2:## +mejoras que peoras\n",
        "            with torch.no_grad():\n",
        "                for param in varianzas_net.parameters():\n",
        "                    param *= (1/0.82)\n",
        "\n",
        "        elif improvements(train_loss) < 2: ## -mejoras que peoras\n",
        "            with torch.no_grad():\n",
        "                for param in varianzas_net.parameters():\n",
        "                    param *= 0.82\n",
        "        \n",
        "        else:\n",
        "            pass\n",
        "    \n",
        "    epoch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iF5e9FeFeHs4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1816\n"
          ]
        }
      ],
      "source": [
        "print(max(test_accuracies))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2.3089392459722977, 2.3089258322583586, 2.3089331533354738, 2.3089323369170556, 2.308943989688654, 2.3089327558017234, 2.3089386578053555, 2.308926501507952, 2.308933406496353, 2.331363138867848, 2.3078895896228393, 2.440253779323879, 2.4666561267015013, 2.79058767127584, 3.3400989626008055, 3.010180982953704, 4.811453668801769, 4.418321354556948, 10.217527754525387, 18.82480004296374, 13.772105462007177, 21.99386759556687, 45.70997622475695, 64.74080042239191, 143.58107632309643, 139.96319021294113, 440.29322326920436, 250.359760536568, 480.15460940363056, 1241.5257479864906, 1448.8484698785649, 738.5807276483793, 1341.848469618287, 3422.6838277397887, 5481.837048105594, 5314.774295636077, 7646.559327733542, 8059.600765529218, 10266.443307839985, 6653.3415038021385, 24605.685163746002, 22956.6271446895, 31916.929951942297, 67563.10882945762, 80467.56999600213, 94896.8792810501, 102030.40131929638, 83748.10826725746, 259069.01502531982, 180015.7687566631, 423983.3984874734, 340118.08542110876, 618480.6978611407, 1582377.279717484, 1636815.6735074627, 2655398.175373134, 7814251.787313432, 7783543.070895523, 11631413.538379531, 19396373.5, 9943119.899253732, 19943941.651385926, 14012415.214285715, 22206098.570362475, 41717716.127931766, 51803078.02558635, 148026528.7249467, 111013050.03837954, 170645208.0, 410362239.7953092, 408784889.108742, 759783975.6759062, 858047580.3837953, 686363286.2089552, 1611410906.8827293, 2899107372.759062, 5741172186.88273, 5303786003.923241, 8005585201.671641, 20448131681.159916, 14777676837.117271, 37840946772.0597, 53399841379.343285, 70894754752.6823, 56246690005.970146, 105860321061.66312, 163637433315.6162, 155117233481.6887, 287582566841.0405, 609847607994.678, 511085438133.2196, 1037723085808.7164, 1488088185238.1067, 3212902660187.7017, 3499725452774.891, 4368545459464.1875, 3139239104293.663, 3515198313452.3496, 2918060936578.456, 4310312895546.951, 5516125744182.584, 4780536014878.567, 11449475783393.979, 21462313752685.168, 41436476554111.18, 85130098774950.48, 63879639762223.484, 121166008089086.9, 91093821268509.47, 276533482815627.75, 485157015466411.94, 418409866812479.3, 665260358218365.5, 1221675696257954.0, 2008120888157854.2, 3930232778518432.0, 6512779911652184.0, 4612498430438734.0, 3727285891790590.5, 8836397084029007.0, 1.992181438925973e+16, 1.7584790379740102e+16, 3.3603808832813076e+16, 5.815456033076418e+16, 8.24335848411928e+16, 1.512593478828391e+17, 2.422307341284315e+17, 2.543873286132961e+17, 1.3998216243933214e+17, 1.7262039970822816e+17, 1.679169940904347e+17, 2.019425288459018e+17, 1.5593448390684003e+17, 2.614475725616868e+17, 4.489855054718858e+17, 1.0858257903497887e+18, 1.7923822515984225e+18, 2.464685663518017e+18, 3.1612679750002063e+18, 3.407828470018807e+18, 1.741570449973206e+18, 1.580828756959122e+18, 1.2873203402968868e+18, 3.0563363345306127e+18, 4.850526808460466e+18, 1.0052099686080324e+19, 1.8702642159358644e+19, 4.451199101510635e+19, 6.292007049047772e+19, 7.755702523354803e+19, 8.955721736540281e+19, 5.884904582906571e+19, 2.492629454798745e+19, 2.6960810276899725e+19, 2.763785481006512e+19, 2.402100994020909e+19, 6.1303826489043436e+19, 1.1556140914583218e+20, 2.0427033055466216e+20]\n"
          ]
        }
      ],
      "source": [
        "print(train_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
