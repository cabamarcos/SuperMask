{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynQ_78kDeHs2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from ast import Param\n",
        "import json\n",
        "\n",
        "from utils.prune import prune_weights\n",
        "from utils.count_improvement import improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC2dq3fNeHs3",
        "outputId": "f25ae62f-1fb0-4f04-ba95-416d32f19c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Verificar si la GPU está disponible y establecer el dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PpJbsf7NeHs3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qzJaokBeHs3"
      },
      "source": [
        "Definimos las redes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8L94OKmYeHs4"
      },
      "outputs": [],
      "source": [
        "net = Net().to(device)\n",
        "varianzas_net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8W9t_3beHs4"
      },
      "source": [
        "Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO_Ykr29eHs4",
        "outputId": "16ad4e8d-dace-485a-dcae-f338879a2c3e"
      },
      "outputs": [],
      "source": [
        "# Definimos el transform para los datos de MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Cargamos el dataset de MNIST\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Definimos los DataLoaders para los conjuntos de entrenamiento y prueba\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cWAORhpoeHs4"
      },
      "outputs": [],
      "source": [
        "# Definimos la función de pérdida para calcular el error\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HTF5fm-peHs4",
        "outputId": "cee40ff8-e105-42b7-a23d-9b5948930430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Train loss: 2.3124109115173566\n",
            "Test accuracy: 0.107\n",
            "Epoch 2\n",
            "Train loss: 2.314133967417898\n",
            "Test accuracy: 0.1051\n",
            "Epoch 3\n",
            "Train loss: 2.320208812064962\n",
            "Test accuracy: 0.0885\n",
            "Epoch 4\n",
            "Train loss: 2.3384816623699947\n",
            "Test accuracy: 0.0976\n",
            "Epoch 5\n",
            "Train loss: 2.3385995130803283\n",
            "Test accuracy: 0.089\n",
            "Epoch 6\n",
            "Train loss: 2.3318909426996193\n",
            "Test accuracy: 0.0631\n",
            "Epoch 7\n",
            "Train loss: 2.346673240539616\n",
            "Test accuracy: 0.1158\n",
            "Epoch 8\n",
            "Train loss: 2.3310187223877734\n",
            "Test accuracy: 0.0611\n",
            "Epoch 9\n",
            "Train loss: 2.3270212330543663\n",
            "Test accuracy: 0.0829\n",
            "Epoch 10\n",
            "Train loss: 2.2748574092189893\n",
            "Test accuracy: 0.119\n",
            "Epoch 11\n",
            "Train loss: 2.3394376077631645\n",
            "Test accuracy: 0.1221\n",
            "Epoch 12\n",
            "Train loss: 2.342031683240618\n",
            "Test accuracy: 0.1218\n",
            "Epoch 13\n",
            "Train loss: 2.508344600958102\n",
            "Test accuracy: 0.1375\n",
            "Epoch 14\n",
            "Train loss: 2.8206646871973455\n",
            "Test accuracy: 0.0874\n",
            "Epoch 15\n",
            "Train loss: 3.7071539158505926\n",
            "Test accuracy: 0.094\n",
            "Epoch 16\n",
            "Train loss: 5.290296971162499\n",
            "Test accuracy: 0.1306\n",
            "Epoch 17\n",
            "Train loss: 7.715165778772155\n",
            "Test accuracy: 0.1217\n",
            "Epoch 18\n",
            "Train loss: 9.30186607995267\n",
            "Test accuracy: 0.0943\n",
            "Epoch 19\n",
            "Train loss: 5.105685087663533\n",
            "Test accuracy: 0.085\n",
            "Epoch 20\n",
            "Train loss: 3.386150807205802\n",
            "Test accuracy: 0.0899\n",
            "Epoch 21\n",
            "Train loss: 3.3965980254256647\n",
            "Test accuracy: 0.0852\n",
            "Epoch 22\n",
            "Train loss: 3.55031141670528\n",
            "Test accuracy: 0.1261\n",
            "Epoch 23\n",
            "Train loss: 2.969250721972126\n",
            "Test accuracy: 0.1235\n",
            "Epoch 24\n",
            "Train loss: 5.144544364292739\n",
            "Test accuracy: 0.1076\n",
            "Epoch 25\n",
            "Train loss: 7.509991573626553\n",
            "Test accuracy: 0.0977\n",
            "Epoch 26\n",
            "Train loss: 13.663575936990506\n",
            "Test accuracy: 0.0907\n",
            "Epoch 27\n",
            "Train loss: 23.72747136585748\n",
            "Test accuracy: 0.0956\n",
            "Epoch 28\n",
            "Train loss: 44.899516982056184\n",
            "Test accuracy: 0.0863\n",
            "Epoch 29\n",
            "Train loss: 54.55224152262023\n",
            "Test accuracy: 0.1253\n",
            "Epoch 30\n",
            "Train loss: 26.72184845328585\n",
            "Test accuracy: 0.1165\n",
            "Epoch 31\n",
            "Train loss: 30.36522963509631\n",
            "Test accuracy: 0.0827\n",
            "Epoch 32\n",
            "Train loss: 26.821275631756162\n",
            "Test accuracy: 0.1278\n",
            "Epoch 33\n",
            "Train loss: 28.19444986599595\n",
            "Test accuracy: 0.1276\n",
            "Epoch 34\n",
            "Train loss: 25.1696468263801\n",
            "Test accuracy: 0.1111\n",
            "Epoch 35\n",
            "Train loss: 64.49274105812187\n",
            "Test accuracy: 0.0874\n",
            "Epoch 36\n",
            "Train loss: 74.88465227806238\n",
            "Test accuracy: 0.1375\n",
            "Epoch 37\n",
            "Train loss: 180.68967257176382\n",
            "Test accuracy: 0.1148\n",
            "Epoch 38\n",
            "Train loss: 201.99150960633494\n",
            "Test accuracy: 0.0794\n",
            "Epoch 39\n",
            "Train loss: 409.3749448861649\n",
            "Test accuracy: 0.0591\n",
            "Epoch 40\n",
            "Train loss: 429.1437556220016\n",
            "Test accuracy: 0.1265\n",
            "Epoch 41\n",
            "Train loss: 653.0474703530513\n",
            "Test accuracy: 0.0863\n",
            "Epoch 42\n",
            "Train loss: 263.6448736943162\n",
            "Test accuracy: 0.1018\n",
            "Epoch 43\n",
            "Train loss: 302.69649503988495\n",
            "Test accuracy: 0.1057\n",
            "Epoch 44\n",
            "Train loss: 142.00860201219507\n",
            "Test accuracy: 0.1101\n",
            "Epoch 45\n",
            "Train loss: 191.53595940238122\n",
            "Test accuracy: 0.1142\n",
            "Epoch 46\n",
            "Train loss: 173.4612186326147\n",
            "Test accuracy: 0.1241\n",
            "Epoch 47\n",
            "Train loss: 275.3068554680993\n",
            "Test accuracy: 0.0832\n",
            "Epoch 48\n",
            "Train loss: 460.86619971301764\n",
            "Test accuracy: 0.0873\n",
            "Epoch 49\n",
            "Train loss: 1002.4037032462895\n",
            "Test accuracy: 0.1277\n",
            "Epoch 50\n",
            "Train loss: 1376.9579302497002\n",
            "Test accuracy: 0.0981\n",
            "Epoch 51\n",
            "Train loss: 2758.986879914046\n",
            "Test accuracy: 0.0867\n",
            "Epoch 52\n",
            "Train loss: 2967.637006877582\n",
            "Test accuracy: 0.0693\n",
            "Epoch 53\n",
            "Train loss: 3132.387172674574\n",
            "Test accuracy: 0.097\n",
            "Epoch 54\n",
            "Train loss: 1412.068226828504\n",
            "Test accuracy: 0.1135\n",
            "Epoch 55\n",
            "Train loss: 1052.1006550412696\n",
            "Test accuracy: 0.1333\n",
            "Epoch 56\n",
            "Train loss: 1477.5108144146038\n",
            "Test accuracy: 0.0876\n",
            "Epoch 57\n",
            "Train loss: 1526.3653949664347\n",
            "Test accuracy: 0.0926\n",
            "Epoch 58\n",
            "Train loss: 1684.270644848789\n",
            "Test accuracy: 0.0737\n",
            "Epoch 59\n",
            "Train loss: 1379.1511780956407\n",
            "Test accuracy: 0.1178\n",
            "Epoch 60\n",
            "Train loss: 3995.5084296187865\n",
            "Test accuracy: 0.0422\n",
            "Epoch 61\n",
            "Train loss: 5722.458660068796\n",
            "Test accuracy: 0.0968\n",
            "Epoch 62\n",
            "Train loss: 12366.506804704159\n",
            "Test accuracy: 0.0683\n",
            "Epoch 63\n",
            "Train loss: 19796.79809518257\n",
            "Test accuracy: 0.1085\n",
            "Epoch 64\n",
            "Train loss: 14800.87377252965\n",
            "Test accuracy: 0.0939\n",
            "Epoch 65\n",
            "Train loss: 18912.154456165044\n",
            "Test accuracy: 0.0765\n",
            "Epoch 66\n",
            "Train loss: 22196.33988747668\n",
            "Test accuracy: 0.1243\n",
            "Epoch 67\n",
            "Train loss: 13519.376500241538\n",
            "Test accuracy: 0.1038\n",
            "Epoch 68\n",
            "Train loss: 31156.929083655385\n",
            "Test accuracy: 0.0802\n",
            "Epoch 69\n",
            "Train loss: 24250.945951742404\n",
            "Test accuracy: 0.0762\n",
            "Epoch 70\n",
            "Train loss: 48835.34671092084\n",
            "Test accuracy: 0.1141\n",
            "Epoch 71\n",
            "Train loss: 92389.66894323028\n",
            "Test accuracy: 0.1182\n",
            "Epoch 72\n",
            "Train loss: 196159.79989005864\n",
            "Test accuracy: 0.0585\n",
            "Epoch 73\n",
            "Train loss: 308195.3540445096\n",
            "Test accuracy: 0.0431\n",
            "Epoch 74\n",
            "Train loss: 339480.43265258527\n",
            "Test accuracy: 0.1289\n",
            "Epoch 75\n",
            "Train loss: 391307.66647787846\n",
            "Test accuracy: 0.0845\n",
            "Epoch 76\n",
            "Train loss: 303746.2980410448\n",
            "Test accuracy: 0.0917\n",
            "Epoch 77\n",
            "Train loss: 397525.6380597015\n",
            "Test accuracy: 0.1005\n",
            "Epoch 78\n",
            "Train loss: 354357.9822094883\n",
            "Test accuracy: 0.0915\n",
            "Epoch 79\n",
            "Train loss: 572235.2235474414\n",
            "Test accuracy: 0.0866\n",
            "Epoch 80\n",
            "Pesos guardados en la época 80 en 'pesos_epoca_80.txt'.\n",
            "Train loss: 321418.6944629531\n",
            "Test accuracy: 0.168\n",
            "Epoch 81\n",
            "Train loss: 961595.8697361407\n",
            "Test accuracy: 0.1016\n",
            "Epoch 82\n",
            "Train loss: 1471883.6317963754\n",
            "Test accuracy: 0.0752\n",
            "Epoch 83\n",
            "Train loss: 1851302.648054371\n",
            "Test accuracy: 0.1011\n",
            "Epoch 84\n",
            "Train loss: 3459896.162313433\n",
            "Test accuracy: 0.1171\n",
            "Epoch 85\n",
            "Train loss: 4834032.612473347\n",
            "Test accuracy: 0.0911\n",
            "Epoch 86\n",
            "Train loss: 7206554.192430704\n",
            "Test accuracy: 0.0566\n",
            "Epoch 87\n",
            "Train loss: 9636194.77771855\n",
            "Test accuracy: 0.1035\n",
            "Epoch 88\n",
            "Train loss: 3514338.2417377396\n",
            "Test accuracy: 0.0988\n",
            "Epoch 89\n",
            "Train loss: 3987758.4848081023\n",
            "Test accuracy: 0.117\n",
            "Epoch 90\n",
            "Train loss: 1872928.4013859276\n",
            "Test accuracy: 0.0751\n",
            "Epoch 91\n",
            "Train loss: 1929809.6163379531\n",
            "Test accuracy: 0.1596\n",
            "Epoch 92\n",
            "Train loss: 1721439.0776918977\n",
            "Test accuracy: 0.1067\n",
            "Epoch 93\n",
            "Train loss: 4301923.054904051\n",
            "Test accuracy: 0.1008\n",
            "Epoch 94\n",
            "Train loss: 8646132.912579957\n",
            "Test accuracy: 0.098\n",
            "Epoch 95\n",
            "Train loss: 9860433.365671642\n",
            "Test accuracy: 0.0935\n",
            "Epoch 96\n",
            "Train loss: 27047441.859275054\n",
            "Test accuracy: 0.1082\n",
            "Epoch 97\n",
            "Train loss: 38821165.108742006\n",
            "Test accuracy: 0.0985\n",
            "Epoch 98\n",
            "Train loss: 48915694.0554371\n",
            "Test accuracy: 0.0819\n",
            "Epoch 99\n",
            "Train loss: 42761063.571428575\n",
            "Test accuracy: 0.104\n",
            "Epoch 100\n",
            "Train loss: 38684506.86567164\n",
            "Test accuracy: 0.0654\n",
            "Epoch 101\n",
            "Train loss: 54324844.81449893\n",
            "Test accuracy: 0.0943\n",
            "Epoch 102\n",
            "Train loss: 78534412.37953092\n",
            "Test accuracy: 0.1179\n",
            "Epoch 103\n",
            "Train loss: 56733461.684434965\n",
            "Test accuracy: 0.0683\n",
            "Epoch 104\n",
            "Train loss: 179540420.69936034\n",
            "Test accuracy: 0.1451\n",
            "Epoch 105\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Pasamos todas las imagenes de train por la red net\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs_net\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpruned_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train loop\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "epoch = 1\n",
        "accuracy_threshold = 0.4\n",
        "\n",
        "\n",
        "while True:\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    if epoch <= 9:\n",
        "\n",
        "        state_dict_red1 = net.state_dict()\n",
        "        state_dict_red2 = varianzas_net.state_dict()\n",
        "\n",
        "        # Crear un nuevo diccionario de estado donde sumamos los pesos\n",
        "        state_dict_suma = {}\n",
        "        for key in state_dict_red1:\n",
        "            if state_dict_red1[key].size() == state_dict_red2[key].size():  # Asegurar que las dimensiones coincidan\n",
        "                # Asegurar que las desviaciones estándar sean positivas para generar el ruido\n",
        "                std_dev = torch.abs(state_dict_red2[key])\n",
        "                \n",
        "                # Generamos los valores aleatorios con una distribución normal usando torch.normal\n",
        "                noise = torch.normal(0, std_dev)  # Media = 0, Desviación estándar = std_dev\n",
        "                \n",
        "                # Crear una máscara para determinar si debemos sumar o restar\n",
        "                mask_negativa = state_dict_red2[key] < 0  # Máscara de valores negativos\n",
        "                \n",
        "                # Aplicar la operación de suma o resta dependiendo de la máscara\n",
        "                state_dict_suma[key] = torch.where(mask_negativa, state_dict_red1[key] - noise, state_dict_red1[key] + noise)\n",
        "            else:\n",
        "                # Si los tamaños no coinciden, copiamos directamente\n",
        "                state_dict_suma[key] = state_dict_red1[key]\n",
        "\n",
        "        # Crear una nueva red o modificar una existente con los pesos sumados\n",
        "        varied_net = Net().to(device)\n",
        "        varied_net.load_state_dict(state_dict_suma)\n",
        "\n",
        "        pruned_net = prune_weights(varied_net)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        # Pasamos todas las imagenes de train por la red net\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs_net = pruned_net(images)\n",
        "            loss = criterion(outputs_net, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss.append(running_loss / len(train_loader))\n",
        "        print(f\"Train loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Evaluamos el modelo en el conjunto de test\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs_net = pruned_net(images)\n",
        "                _, predicted = torch.max(outputs_net.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracies.append(correct / total)\n",
        "        print(f\"Test accuracy: {correct / total}\")\n",
        "\n",
        "        if correct / total > accuracy_threshold:\n",
        "            break\n",
        "\n",
        "    else:\n",
        "        # Modificación: Guardar los pesos en la época 80\n",
        "        if epoch == 80:\n",
        "            # Convertir el state_dict a un formato que se pueda escribir en un archivo txt\n",
        "            state_dict_suma_serializable = {key: value.cpu().numpy().tolist() for key, value in state_dict_suma.items()}\n",
        "            # Convertir el state_dict a un formato que se pueda escribir en un archivo txt\n",
        "            state_dict_var_serializable = {key: value.cpu().numpy().tolist() for key, value in state_dict_red2.items()}\n",
        "\n",
        "            # Guardar en un archivo de texto\n",
        "            with open(\"pesos_epoca_80.txt\", \"w\") as f:\n",
        "                json.dump(state_dict_suma_serializable, f)\n",
        "\n",
        "            with open(\"varianzas_epoca_80.txt\", \"w\") as f:\n",
        "                json.dump(state_dict_var_serializable, f)\n",
        "\n",
        "            print(f\"Pesos guardados en la época 80 en 'pesos_epoca_80.txt'.\")\n",
        "\n",
        "        # Sumamos los pesos de las redes en una red nueva\n",
        "        state_dict_red1 = net.state_dict()\n",
        "        state_dict_red2 = varianzas_net.state_dict()\n",
        "\n",
        "        state_dict_suma = {}\n",
        "        for key in state_dict_red1:\n",
        "            if state_dict_red1[key].size() == state_dict_red2[key].size():  # Asegurar que las dimensiones coincidan\n",
        "                # Asegurar que las desviaciones estándar sean positivas para generar el ruido\n",
        "                std_dev = torch.abs(state_dict_red2[key])\n",
        "                \n",
        "                # Generamos los valores aleatorios con una distribución normal usando torch.normal\n",
        "                noise = torch.normal(0, std_dev)  # Media = 0, Desviación estándar = std_dev\n",
        "                \n",
        "                # Crear una máscara para determinar si debemos sumar o restar\n",
        "                mask_negativa = state_dict_red2[key] < 0  # Máscara de valores negativos\n",
        "                \n",
        "                # Aplicar la operación de suma o resta dependiendo de la máscara\n",
        "                state_dict_suma[key] = torch.where(mask_negativa, state_dict_red1[key] - noise, state_dict_red1[key] + noise)\n",
        "            else:\n",
        "                # Si los tamaños no coinciden, copiamos directamente\n",
        "                state_dict_suma[key] = state_dict_red1[key]\n",
        "\n",
        "\n",
        "        # Crear una nueva red o modificar una existente con los pesos sumados\n",
        "        varied_net = Net().to(device)\n",
        "        varied_net.load_state_dict(state_dict_suma)\n",
        "\n",
        "        #print(varied_net.state_dict())\n",
        "\n",
        "        pruned_net = prune_weights(varied_net)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        # Pasamos todas las imagenes de train por la red net\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs_net = pruned_net(images)\n",
        "            loss = criterion(outputs_net, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss.append(running_loss / len(train_loader))\n",
        "        print(f\"Train loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Evaluamos el modelo en el conjunto de test\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs_net = pruned_net(images)\n",
        "                _, predicted = torch.max(outputs_net.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_accuracies.append(correct / total)\n",
        "        print(f\"Test accuracy: {correct / total}\")\n",
        "\n",
        "        if correct / total > accuracy_threshold:\n",
        "            break\n",
        "\n",
        "        # actualizmos el vector de varianzas\n",
        "        if improvements(train_loss) > 2:## +mejoras que peoras\n",
        "            with torch.no_grad():\n",
        "                for param in varianzas_net.parameters():\n",
        "                    param *= (1/0.82)\n",
        "\n",
        "        elif improvements(train_loss) < 2: ## -mejoras que peoras\n",
        "            with torch.no_grad():\n",
        "                for param in varianzas_net.parameters():\n",
        "                    param *= 0.82\n",
        "        \n",
        "        else:\n",
        "            pass\n",
        "    \n",
        "    epoch += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iF5e9FeFeHs4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1692\n"
          ]
        }
      ],
      "source": [
        "print(max(test_accuracies))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2.3089392459722977, 2.3089258322583586, 2.3089331533354738, 2.3089323369170556, 2.308943989688654, 2.3089327558017234, 2.3089386578053555, 2.308926501507952, 2.308933406496353, 2.331363138867848, 2.3078895896228393, 2.440253779323879, 2.4666561267015013, 2.79058767127584, 3.3400989626008055, 3.010180982953704, 4.811453668801769, 4.418321354556948, 10.217527754525387, 18.82480004296374, 13.772105462007177, 21.99386759556687, 45.70997622475695, 64.74080042239191, 143.58107632309643, 139.96319021294113, 440.29322326920436, 250.359760536568, 480.15460940363056, 1241.5257479864906, 1448.8484698785649, 738.5807276483793, 1341.848469618287, 3422.6838277397887, 5481.837048105594, 5314.774295636077, 7646.559327733542, 8059.600765529218, 10266.443307839985, 6653.3415038021385, 24605.685163746002, 22956.6271446895, 31916.929951942297, 67563.10882945762, 80467.56999600213, 94896.8792810501, 102030.40131929638, 83748.10826725746, 259069.01502531982, 180015.7687566631, 423983.3984874734, 340118.08542110876, 618480.6978611407, 1582377.279717484, 1636815.6735074627, 2655398.175373134, 7814251.787313432, 7783543.070895523, 11631413.538379531, 19396373.5, 9943119.899253732, 19943941.651385926, 14012415.214285715, 22206098.570362475, 41717716.127931766, 51803078.02558635, 148026528.7249467, 111013050.03837954, 170645208.0, 410362239.7953092, 408784889.108742, 759783975.6759062, 858047580.3837953, 686363286.2089552, 1611410906.8827293, 2899107372.759062, 5741172186.88273, 5303786003.923241, 8005585201.671641, 20448131681.159916, 14777676837.117271, 37840946772.0597, 53399841379.343285, 70894754752.6823, 56246690005.970146, 105860321061.66312, 163637433315.6162, 155117233481.6887, 287582566841.0405, 609847607994.678, 511085438133.2196, 1037723085808.7164, 1488088185238.1067, 3212902660187.7017, 3499725452774.891, 4368545459464.1875, 3139239104293.663, 3515198313452.3496, 2918060936578.456, 4310312895546.951, 5516125744182.584, 4780536014878.567, 11449475783393.979, 21462313752685.168, 41436476554111.18, 85130098774950.48, 63879639762223.484, 121166008089086.9, 91093821268509.47, 276533482815627.75, 485157015466411.94, 418409866812479.3, 665260358218365.5, 1221675696257954.0, 2008120888157854.2, 3930232778518432.0, 6512779911652184.0, 4612498430438734.0, 3727285891790590.5, 8836397084029007.0, 1.992181438925973e+16, 1.7584790379740102e+16, 3.3603808832813076e+16, 5.815456033076418e+16, 8.24335848411928e+16, 1.512593478828391e+17, 2.422307341284315e+17, 2.543873286132961e+17, 1.3998216243933214e+17, 1.7262039970822816e+17, 1.679169940904347e+17, 2.019425288459018e+17, 1.5593448390684003e+17, 2.614475725616868e+17, 4.489855054718858e+17, 1.0858257903497887e+18, 1.7923822515984225e+18, 2.464685663518017e+18, 3.1612679750002063e+18, 3.407828470018807e+18, 1.741570449973206e+18, 1.580828756959122e+18, 1.2873203402968868e+18, 3.0563363345306127e+18, 4.850526808460466e+18, 1.0052099686080324e+19, 1.8702642159358644e+19, 4.451199101510635e+19, 6.292007049047772e+19, 7.755702523354803e+19, 8.955721736540281e+19, 5.884904582906571e+19, 2.492629454798745e+19, 2.6960810276899725e+19, 2.763785481006512e+19, 2.402100994020909e+19, 6.1303826489043436e+19, 1.1556140914583218e+20, 2.0427033055466216e+20]\n"
          ]
        }
      ],
      "source": [
        "print(train_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
