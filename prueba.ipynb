{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMWCMXbHihNjSEqwW3G05My",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cabamarcos/SuperMask/blob/main/prueba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Z2ygqOcxdDYu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si la GPU está disponible y establecer el dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTQtMKeHdG7v",
        "outputId": "bcafe481-d61b-4f01-eb99-feff5279fbc2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos las redes"
      ],
      "metadata": {
        "id": "QcjIzXRtgqio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las dos redes convolucionales\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Mask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mask, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Inicializamos las dos redes y las movemos a la GPU si está disponible\n",
        "net = Net().to(device)\n",
        "mask = Mask().to(device)"
      ],
      "metadata": {
        "id": "bezqZdd2dL1W"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos"
      ],
      "metadata": {
        "id": "5g4KnfNSgwKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el transform para los datos de MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Cargamos el dataset de MNIST\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Definimos los DataLoaders para los conjuntos de entrenamiento y prueba\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "8YyUD8ijdQue"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la función de pérdida y los optimizadores\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_mask = optim.SGD(mask.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "-yvVNoDwdWYH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_parameters(layer_name, params):\n",
        "    print(f\"--- {layer_name} ---\")\n",
        "    print(params)\n",
        "\n",
        "def apply_mask(net, mask):\n",
        "    # Aplica la máscara a la red: selecciona el 30% de los pesos más altos y desactiva el resto\n",
        "    net_masked = copy.deepcopy(net)\n",
        "    with torch.no_grad():\n",
        "        for net_name, net_param, mask_name, mask_param in zip(net_masked.state_dict(), net_masked.parameters(), mask.state_dict(), mask.parameters()):\n",
        "            #print_parameters(\"Mask before applying\", mask_param.data)\n",
        "            #print_parameters(\"Net before applying mask\", net_param.data)\n",
        "\n",
        "            mask_data = mask_param.data.abs()\n",
        "            threshold = torch.quantile(mask_data, 0.7)\n",
        "            mask_applied = (mask_data >= threshold).float()\n",
        "\n",
        "            #print_parameters(\"Mask applied (binary)\", mask_applied)\n",
        "            net_param.data *= mask_applied\n",
        "\n",
        "            #print_parameters(\"Net after applying mask\", net_param.data)\n",
        "            #print(\"\\n\")\n",
        "\n",
        "    return net_masked\n"
      ],
      "metadata": {
        "id": "pQ_XpdADdYMH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Param\n",
        "# Listas para almacenar las pérdidas de entrenamiento y las precisiones de validación\n",
        "train_loss = []\n",
        "test_accuracies = []\n",
        "epochs = 10\n",
        "accuracy_threshold = 0.6\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Entrenamiento de la red\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "\n",
        "    # Establecemos el modo de entrenamiento\n",
        "    net.train()\n",
        "    mask.train()\n",
        "\n",
        "\n",
        "    #print(\"Net before mask: \", net.state_dict())\n",
        "    #print(\"Mask before: \", mask.state_dict())\n",
        "\n",
        "    # Aplicamos la máscara a la red\n",
        "    net_masked = apply_mask(net, mask)\n",
        "    #print(\"net_masked: \", net_masked.state_dict())\n",
        "    #print(\"Net after mask: \", net.state_dict())\n",
        "    print(\"Mask after: \", mask.state_dict())\n",
        "\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      # Paso 1: Pasar imágenes por la red net y calcular la pérdida sin retropropagarla\n",
        "      outputs_net = net_masked(images)\n",
        "      loss = criterion(outputs_net, labels)\n",
        "\n",
        "      # Paso 2: Retropropagar la pérdida con respecto a los parámetros de la red mask\n",
        "      optimizer_mask.zero_grad()  # Limpiar los gradientes acumulados de la máscara\n",
        "      outputs_mask = mask(images)  # Obtener la salida de la máscara (ficticio)\n",
        "      # Calcular el loss de mask borrando el loss real de mask\n",
        "      loss_mask = criterion(outputs_mask, labels)*0.0  + loss\n",
        "      loss_mask.backward(retain_graph=True)  # Calcular gradientes\n",
        "      #grad_mask = torch.autograd.grad(loss_mask, mask.parameters(), create_graph=True, allow_unused=True) # Retropropagar gradientes\n",
        "\n",
        "\n",
        "\n",
        "      #Actualizamos los gradientes manualmente\n",
        "      #for name, param in mask.named_parameters():\n",
        "       # if param.grad is not None:\n",
        "        #  param.grad = param.grad.clone()\n",
        "\n",
        "\n",
        "      optimizer_mask.step()  # Actualizar los parámetros de la máscara\n",
        "\n",
        "      # Limpiamos gradientes para la proxima iteraccion\n",
        "      net_masked.zero_grad()\n",
        "      mask.zero_grad()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    train_loss.append(running_loss / len(train_loader))\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "\n",
        "    # Verificar que los parámetros de NetB se han actualizado\n",
        "    for name, param in mask.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(name, param.data)\n",
        "########################################\n",
        "\n",
        "\n",
        "    # test de la red\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    test_accuracies.append(accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}, test Accuracy: {accuracy}')\n",
        "\n",
        "    # Paramos el entrenamiento si la precisión en validación supera el 60%\n",
        "    if accuracy > accuracy_threshold:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UhTaCUdHddkP",
        "outputId": "47b83f59-4cf8-4f0f-cb3c-e00aef336680"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Mask after:  OrderedDict([('conv1.weight', tensor([[[[-3.8170e-02,  1.7448e-01,  2.3496e-01],\n",
            "          [-1.5860e-01,  1.1333e-01,  2.5293e-01],\n",
            "          [-2.6785e-01, -2.3868e-01, -2.1758e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.2197e-01,  2.9401e-01, -1.1885e-01],\n",
            "          [-2.6830e-01,  2.3187e-01, -1.1921e-01],\n",
            "          [-4.9437e-03, -2.3244e-02, -1.5611e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.1819e-01, -1.5778e-01, -1.2459e-01],\n",
            "          [-1.6847e-01,  2.0059e-02,  5.5659e-02],\n",
            "          [ 3.4198e-02,  6.4652e-02, -2.0674e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1781e-01,  3.7451e-02,  1.4201e-01],\n",
            "          [ 3.0368e-01,  2.5538e-01,  2.7984e-02],\n",
            "          [-3.2209e-01, -2.4329e-02, -1.7965e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8919e-01,  3.0817e-01, -1.1320e-01],\n",
            "          [ 5.1017e-02, -2.8216e-01,  1.6445e-01],\n",
            "          [-1.8167e-01,  1.1946e-01,  2.6334e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.6361e-01,  1.4705e-01, -3.0217e-01],\n",
            "          [-1.9577e-01,  1.1305e-01, -2.5796e-01],\n",
            "          [-1.9008e-01, -2.6434e-01, -2.7964e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1207e-01,  7.1084e-02, -2.6944e-01],\n",
            "          [ 3.1963e-01, -1.5505e-01, -2.7915e-01],\n",
            "          [-6.7942e-02,  1.6930e-01,  3.2340e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5427e-01, -1.1633e-01, -3.0335e-01],\n",
            "          [ 1.6527e-01,  1.2365e-02, -1.3192e-01],\n",
            "          [-3.1019e-01, -1.4157e-01,  2.6828e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.6890e-01,  2.9589e-01, -2.1955e-02],\n",
            "          [-1.8293e-01, -1.9428e-01,  6.0233e-02],\n",
            "          [ 2.9277e-01, -2.7975e-02,  1.9270e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2326e-01,  2.5571e-01, -2.6386e-01],\n",
            "          [ 8.3422e-02,  3.3134e-02, -3.1233e-01],\n",
            "          [ 5.2966e-02, -1.2890e-01, -2.1417e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9762e-01, -2.9438e-01, -1.7060e-02],\n",
            "          [ 2.9119e-01, -1.4083e-01, -2.5495e-01],\n",
            "          [-1.9626e-01, -8.8423e-02,  3.3119e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2838e-01,  1.4757e-01, -2.2976e-01],\n",
            "          [-2.0539e-01, -9.1156e-02,  2.3059e-01],\n",
            "          [-1.9702e-01,  2.7323e-01,  1.4067e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7351e-01, -6.6939e-03,  2.6979e-01],\n",
            "          [-9.3695e-02,  1.9856e-01,  7.0799e-02],\n",
            "          [ 2.1031e-02, -2.2877e-02, -2.9215e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.6560e-01, -2.6585e-01,  2.2244e-01],\n",
            "          [-1.1545e-01,  3.1470e-01, -7.7343e-02],\n",
            "          [ 3.3263e-01,  1.9844e-01,  2.5977e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2062e-02, -2.0562e-01, -6.3328e-02],\n",
            "          [ 1.0219e-01, -9.7934e-02,  1.2371e-01],\n",
            "          [ 2.3341e-01, -1.2882e-01, -1.4685e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9748e-01, -1.9512e-01, -3.0253e-01],\n",
            "          [ 1.7277e-01, -3.0177e-01,  6.5577e-02],\n",
            "          [ 2.6709e-02,  7.7624e-02, -2.3677e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.1628e-01,  1.9117e-01,  1.1065e-02],\n",
            "          [-1.2876e-01, -2.0072e-01, -1.4530e-01],\n",
            "          [ 1.3208e-01,  1.2711e-01, -1.0924e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2636e-01, -1.8376e-01,  1.7084e-01],\n",
            "          [-2.2380e-01, -7.9693e-02,  5.2609e-02],\n",
            "          [-8.3039e-02, -2.3608e-01,  2.3673e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.1669e-01,  1.5097e-01, -5.9881e-02],\n",
            "          [-5.5988e-02, -9.4613e-02, -3.2287e-02],\n",
            "          [ 2.7249e-01,  3.0703e-01,  2.3113e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.1636e-02, -2.3264e-02,  2.1763e-01],\n",
            "          [-7.8779e-02,  2.6219e-01,  2.6635e-01],\n",
            "          [ 3.6628e-02,  3.2162e-01,  4.4587e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9343e-02, -2.4690e-01, -1.0746e-01],\n",
            "          [-2.5761e-01, -6.8864e-02, -2.6722e-01],\n",
            "          [ 1.5131e-01,  2.4796e-01, -2.6010e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1780e-01,  2.2181e-01,  5.2403e-02],\n",
            "          [ 2.0024e-01,  1.9113e-01,  2.7417e-01],\n",
            "          [ 3.3799e-02, -2.2864e-01, -1.7448e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6237e-01, -1.5079e-01, -6.8724e-02],\n",
            "          [-2.1377e-02,  7.1947e-02, -5.6126e-02],\n",
            "          [ 7.5524e-02, -3.2647e-02, -1.3470e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.3924e-02, -2.2574e-01, -1.4479e-01],\n",
            "          [-1.8964e-01, -6.6128e-02,  3.0624e-01],\n",
            "          [ 1.2443e-01,  1.7347e-01,  4.9810e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.3448e-02, -2.1600e-01, -1.9656e-01],\n",
            "          [-1.4544e-01, -2.5835e-01,  1.5870e-01],\n",
            "          [ 1.9680e-01,  2.9870e-01,  2.8283e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.6859e-01, -4.2042e-03,  1.6863e-01],\n",
            "          [-1.7903e-01,  1.2118e-01, -2.7681e-01],\n",
            "          [ 1.1989e-01, -2.7146e-01,  1.0221e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.1260e-01, -3.3026e-01,  2.7215e-01],\n",
            "          [-3.1655e-01, -2.1556e-01, -3.2643e-04],\n",
            "          [-3.1557e-01, -2.0924e-01,  2.6490e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 5.3455e-02, -1.9432e-01, -2.8289e-02],\n",
            "          [ 3.1121e-01, -1.0580e-01, -6.9330e-02],\n",
            "          [-6.0609e-02, -1.5718e-01,  2.3881e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.1869e-01, -2.4085e-01, -1.0267e-01],\n",
            "          [-1.1169e-01,  3.1843e-01, -1.5525e-01],\n",
            "          [ 4.6709e-02, -2.5017e-01,  1.9627e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.2748e-01,  3.3054e-01, -2.5561e-01],\n",
            "          [ 4.4279e-02, -1.7770e-01, -2.8399e-01],\n",
            "          [-2.0939e-01,  2.7563e-01, -1.8559e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 6.5947e-03, -4.4017e-02, -1.4437e-01],\n",
            "          [-1.4325e-01,  2.8939e-01,  2.8594e-01],\n",
            "          [ 3.5126e-02, -3.1710e-01,  2.7811e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.1300e-01,  2.5135e-01,  9.5140e-02],\n",
            "          [-3.2542e-02, -1.3308e-01, -3.7343e-02],\n",
            "          [-1.6205e-01, -2.8033e-01,  1.6624e-01]]]])), ('conv1.bias', tensor([-0.1185,  0.1878,  0.1493,  0.0908, -0.1899,  0.2949, -0.2927, -0.2888,\n",
            "         0.1920, -0.2750,  0.2409,  0.1394,  0.1215,  0.2013, -0.2471,  0.1664,\n",
            "        -0.1017, -0.2305,  0.3225, -0.3273,  0.1468, -0.2528,  0.1590,  0.3053,\n",
            "         0.3195,  0.1774,  0.0099,  0.2114, -0.3082,  0.2834,  0.1919,  0.0396])), ('conv2.weight', tensor([[[[-0.0126,  0.0138, -0.0012],\n",
            "          [-0.0007, -0.0301, -0.0412],\n",
            "          [-0.0177,  0.0063, -0.0546]],\n",
            "\n",
            "         [[ 0.0394, -0.0053, -0.0430],\n",
            "          [ 0.0248,  0.0033,  0.0577],\n",
            "          [-0.0357, -0.0358,  0.0546]],\n",
            "\n",
            "         [[-0.0022, -0.0187, -0.0586],\n",
            "          [-0.0519, -0.0465, -0.0040],\n",
            "          [ 0.0088,  0.0350, -0.0194]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0489, -0.0540, -0.0022],\n",
            "          [-0.0274, -0.0345, -0.0569],\n",
            "          [ 0.0467, -0.0190, -0.0018]],\n",
            "\n",
            "         [[ 0.0418,  0.0283, -0.0587],\n",
            "          [ 0.0117, -0.0161,  0.0303],\n",
            "          [-0.0321, -0.0517, -0.0433]],\n",
            "\n",
            "         [[ 0.0151, -0.0382, -0.0275],\n",
            "          [-0.0274, -0.0126, -0.0355],\n",
            "          [-0.0164,  0.0181,  0.0005]]],\n",
            "\n",
            "\n",
            "        [[[-0.0257,  0.0052, -0.0431],\n",
            "          [-0.0487,  0.0239,  0.0363],\n",
            "          [-0.0123,  0.0546, -0.0229]],\n",
            "\n",
            "         [[ 0.0242,  0.0211,  0.0517],\n",
            "          [-0.0124, -0.0491, -0.0362],\n",
            "          [ 0.0014, -0.0075,  0.0161]],\n",
            "\n",
            "         [[-0.0522,  0.0523, -0.0493],\n",
            "          [ 0.0097,  0.0041,  0.0420],\n",
            "          [ 0.0294, -0.0129,  0.0116]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0179,  0.0539, -0.0359],\n",
            "          [-0.0285,  0.0134, -0.0303],\n",
            "          [ 0.0421,  0.0133, -0.0583]],\n",
            "\n",
            "         [[ 0.0575,  0.0193, -0.0391],\n",
            "          [-0.0084,  0.0316,  0.0123],\n",
            "          [ 0.0048,  0.0226,  0.0423]],\n",
            "\n",
            "         [[ 0.0565,  0.0325, -0.0584],\n",
            "          [-0.0543,  0.0289,  0.0524],\n",
            "          [ 0.0207,  0.0562,  0.0269]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0496, -0.0346,  0.0319],\n",
            "          [ 0.0350, -0.0253, -0.0588],\n",
            "          [ 0.0299, -0.0421,  0.0131]],\n",
            "\n",
            "         [[-0.0156, -0.0342, -0.0407],\n",
            "          [ 0.0216,  0.0367, -0.0519],\n",
            "          [ 0.0145,  0.0170,  0.0446]],\n",
            "\n",
            "         [[ 0.0226,  0.0354,  0.0184],\n",
            "          [-0.0264, -0.0363, -0.0346],\n",
            "          [ 0.0353,  0.0116, -0.0052]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0530,  0.0147,  0.0116],\n",
            "          [ 0.0452, -0.0490,  0.0248],\n",
            "          [ 0.0480,  0.0347,  0.0276]],\n",
            "\n",
            "         [[-0.0029,  0.0007, -0.0397],\n",
            "          [ 0.0349,  0.0466,  0.0352],\n",
            "          [ 0.0429, -0.0350, -0.0216]],\n",
            "\n",
            "         [[-0.0459, -0.0399,  0.0419],\n",
            "          [ 0.0035,  0.0473, -0.0572],\n",
            "          [ 0.0054,  0.0162, -0.0175]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0094, -0.0204,  0.0391],\n",
            "          [ 0.0414, -0.0494, -0.0115],\n",
            "          [ 0.0184,  0.0058, -0.0522]],\n",
            "\n",
            "         [[-0.0102,  0.0339,  0.0377],\n",
            "          [-0.0449, -0.0563,  0.0356],\n",
            "          [-0.0150,  0.0195,  0.0308]],\n",
            "\n",
            "         [[-0.0112,  0.0092, -0.0076],\n",
            "          [ 0.0497,  0.0446, -0.0358],\n",
            "          [-0.0277,  0.0155, -0.0220]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0125,  0.0575,  0.0421],\n",
            "          [ 0.0572, -0.0430,  0.0422],\n",
            "          [ 0.0396,  0.0030,  0.0587]],\n",
            "\n",
            "         [[-0.0381,  0.0003, -0.0366],\n",
            "          [ 0.0024,  0.0377, -0.0242],\n",
            "          [-0.0196,  0.0164,  0.0008]],\n",
            "\n",
            "         [[-0.0153, -0.0200,  0.0529],\n",
            "          [ 0.0056, -0.0122,  0.0064],\n",
            "          [ 0.0066,  0.0142,  0.0073]]],\n",
            "\n",
            "\n",
            "        [[[-0.0435,  0.0061, -0.0466],\n",
            "          [ 0.0266,  0.0454,  0.0013],\n",
            "          [-0.0560, -0.0156,  0.0578]],\n",
            "\n",
            "         [[ 0.0040, -0.0414, -0.0260],\n",
            "          [ 0.0213,  0.0200, -0.0020],\n",
            "          [ 0.0275,  0.0263,  0.0134]],\n",
            "\n",
            "         [[ 0.0260,  0.0002, -0.0500],\n",
            "          [ 0.0045, -0.0130, -0.0384],\n",
            "          [-0.0246,  0.0510,  0.0576]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0124, -0.0229, -0.0438],\n",
            "          [-0.0541,  0.0225,  0.0475],\n",
            "          [ 0.0266, -0.0033, -0.0198]],\n",
            "\n",
            "         [[ 0.0035,  0.0360, -0.0189],\n",
            "          [ 0.0419,  0.0170,  0.0540],\n",
            "          [ 0.0017,  0.0048, -0.0085]],\n",
            "\n",
            "         [[ 0.0390, -0.0148,  0.0548],\n",
            "          [ 0.0519,  0.0219,  0.0516],\n",
            "          [-0.0221, -0.0073,  0.0085]]],\n",
            "\n",
            "\n",
            "        [[[-0.0401, -0.0136, -0.0388],\n",
            "          [-0.0239,  0.0526,  0.0086],\n",
            "          [ 0.0501,  0.0216, -0.0184]],\n",
            "\n",
            "         [[ 0.0542, -0.0122,  0.0204],\n",
            "          [-0.0193, -0.0283,  0.0180],\n",
            "          [ 0.0484, -0.0147,  0.0488]],\n",
            "\n",
            "         [[ 0.0475,  0.0416, -0.0397],\n",
            "          [-0.0580,  0.0304, -0.0564],\n",
            "          [-0.0389,  0.0500,  0.0272]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0235, -0.0215, -0.0082],\n",
            "          [ 0.0157, -0.0151, -0.0124],\n",
            "          [-0.0186,  0.0292, -0.0463]],\n",
            "\n",
            "         [[-0.0504,  0.0528,  0.0403],\n",
            "          [-0.0547, -0.0271,  0.0513],\n",
            "          [ 0.0237, -0.0147,  0.0444]],\n",
            "\n",
            "         [[ 0.0375, -0.0255,  0.0213],\n",
            "          [-0.0278,  0.0560,  0.0298],\n",
            "          [ 0.0164,  0.0312, -0.0152]]]])), ('conv2.bias', tensor([ 0.0224,  0.0081,  0.0016,  0.0028,  0.0335, -0.0568, -0.0434,  0.0496,\n",
            "        -0.0310,  0.0185,  0.0211,  0.0568, -0.0560, -0.0048,  0.0424, -0.0266,\n",
            "         0.0360,  0.0283, -0.0460, -0.0054,  0.0351,  0.0266,  0.0423, -0.0245,\n",
            "        -0.0460, -0.0033,  0.0095, -0.0106,  0.0042,  0.0363,  0.0272, -0.0203,\n",
            "         0.0588,  0.0383,  0.0074,  0.0077, -0.0393,  0.0445,  0.0121,  0.0054,\n",
            "         0.0128,  0.0386,  0.0372, -0.0391,  0.0513,  0.0580, -0.0458, -0.0209,\n",
            "         0.0486,  0.0019, -0.0110,  0.0356, -0.0141,  0.0169, -0.0125, -0.0345,\n",
            "        -0.0203, -0.0415,  0.0316,  0.0470, -0.0039,  0.0190,  0.0454,  0.0444])), ('fc1.weight', tensor([[ 0.0022,  0.0105, -0.0046,  ...,  0.0071, -0.0120, -0.0240],\n",
            "        [ 0.0160, -0.0066,  0.0245,  ..., -0.0234, -0.0171,  0.0171],\n",
            "        [-0.0085, -0.0207,  0.0001,  ...,  0.0127,  0.0199,  0.0054],\n",
            "        ...,\n",
            "        [-0.0062,  0.0240,  0.0097,  ...,  0.0106,  0.0077,  0.0051],\n",
            "        [-0.0025, -0.0212,  0.0102,  ..., -0.0212,  0.0050,  0.0236],\n",
            "        [ 0.0108, -0.0025,  0.0191,  ..., -0.0157,  0.0088,  0.0064]])), ('fc1.bias', tensor([-0.0137,  0.0116,  0.0140,  0.0123, -0.0094,  0.0048, -0.0141,  0.0057,\n",
            "        -0.0034,  0.0101,  0.0170,  0.0041,  0.0108,  0.0107, -0.0150,  0.0012,\n",
            "        -0.0231,  0.0080,  0.0245,  0.0003, -0.0183, -0.0049, -0.0088,  0.0207,\n",
            "         0.0012,  0.0028, -0.0076,  0.0007,  0.0189,  0.0016,  0.0117, -0.0199,\n",
            "         0.0090,  0.0188, -0.0245,  0.0174,  0.0165, -0.0178,  0.0238,  0.0019,\n",
            "         0.0009,  0.0192, -0.0036, -0.0096,  0.0128, -0.0046,  0.0027, -0.0167,\n",
            "        -0.0161, -0.0102, -0.0118,  0.0023, -0.0138,  0.0125, -0.0208, -0.0183,\n",
            "         0.0015,  0.0221,  0.0070, -0.0023,  0.0117,  0.0005, -0.0048,  0.0131])), ('fc2.weight', tensor([[-9.6611e-02,  1.0865e-01, -7.3420e-02, -2.7554e-02, -3.2121e-02,\n",
            "         -2.2039e-02, -9.8088e-02, -1.0543e-01,  3.8566e-02, -5.6531e-02,\n",
            "         -9.1681e-02,  3.4307e-03, -6.5868e-02,  1.7550e-02,  1.0185e-01,\n",
            "         -1.1752e-01, -3.9907e-02, -1.1229e-01,  2.2817e-02,  2.4463e-03,\n",
            "          1.1575e-01, -5.3404e-02,  7.6461e-02, -7.7798e-02, -1.2305e-01,\n",
            "          1.0114e-01,  3.5144e-02, -1.0645e-01, -7.8905e-02,  3.1283e-02,\n",
            "          1.6139e-02,  5.5531e-02,  3.6384e-02,  1.0170e-01,  2.8752e-02,\n",
            "         -8.9039e-02, -1.2318e-02, -1.0625e-02,  5.2603e-02,  8.2390e-02,\n",
            "          1.0619e-01, -9.7745e-02, -1.3613e-02,  1.0319e-02, -6.0451e-02,\n",
            "          7.5740e-02, -6.6683e-02, -2.4215e-02, -3.2339e-02, -6.8414e-02,\n",
            "          4.3916e-02,  7.5960e-02, -7.9935e-02,  8.7895e-02,  8.4854e-02,\n",
            "          9.0814e-02, -4.8558e-02,  3.8199e-02, -3.1081e-03,  8.3908e-02,\n",
            "         -6.5105e-02,  9.5080e-02, -3.5865e-02, -2.6242e-02],\n",
            "        [-1.1697e-01,  6.1898e-02, -5.2192e-02,  4.4864e-02, -9.6314e-02,\n",
            "          3.6091e-02,  5.4595e-03,  9.4346e-02, -2.9835e-02, -3.9526e-02,\n",
            "          1.1903e-01, -5.9223e-02,  1.5571e-02,  4.9057e-02,  3.4747e-02,\n",
            "         -1.1354e-01, -1.1098e-03, -7.9803e-02, -4.8983e-02,  1.9356e-02,\n",
            "          3.1213e-03,  5.0844e-03,  7.1797e-02, -6.9728e-02, -5.1679e-02,\n",
            "         -1.0630e-01,  3.3274e-02, -1.1889e-01,  7.3314e-02,  4.1293e-02,\n",
            "         -2.0265e-02, -1.8293e-02, -9.1129e-02,  7.3375e-02,  1.9335e-02,\n",
            "         -1.4784e-02, -6.3958e-02,  5.5783e-02, -5.8513e-02,  1.1408e-01,\n",
            "          1.0426e-01, -6.9669e-02, -7.8907e-02,  1.1654e-01, -7.2292e-02,\n",
            "          1.4250e-02,  9.4463e-02, -5.2524e-02,  6.1910e-02,  1.1137e-04,\n",
            "          6.3955e-02,  8.4280e-02,  4.8882e-02, -1.0960e-01,  1.5656e-02,\n",
            "          9.1442e-02, -3.9323e-02, -5.1351e-02,  5.1981e-02,  7.8918e-02,\n",
            "          8.0820e-02,  5.2254e-02, -5.0654e-02,  7.4453e-02],\n",
            "        [ 1.6801e-02, -1.1677e-01,  1.0686e-01, -1.1040e-01, -5.0007e-04,\n",
            "          1.9411e-02, -8.9388e-02, -1.2333e-01, -7.3066e-02, -5.0863e-02,\n",
            "          8.7147e-02,  4.6207e-02, -1.2609e-02, -5.9017e-02,  7.3747e-02,\n",
            "         -3.7694e-02,  5.5440e-02,  2.8656e-02,  1.0466e-01, -6.2059e-02,\n",
            "          9.9169e-04, -2.5638e-02, -7.6647e-02,  1.1668e-01, -4.1713e-02,\n",
            "         -1.5911e-02, -1.1137e-01, -4.9218e-02,  9.5917e-02, -8.2595e-02,\n",
            "          1.1271e-01,  5.3275e-02, -2.1036e-02, -7.8650e-02, -1.1252e-01,\n",
            "         -3.5749e-03,  1.8029e-02,  9.8136e-02,  1.1176e-01, -1.1781e-01,\n",
            "         -4.8779e-02,  4.4182e-02,  1.2225e-01,  9.2703e-04,  2.1808e-02,\n",
            "          1.1918e-01,  1.0052e-01,  1.0529e-01, -1.1317e-01,  6.9600e-02,\n",
            "          1.1744e-01,  5.9968e-03, -1.1048e-01,  9.1522e-02,  2.4719e-02,\n",
            "         -6.8849e-02,  1.0875e-01, -9.9488e-02,  1.4229e-02,  8.5769e-02,\n",
            "          8.0198e-02, -1.1004e-01, -2.4122e-02,  3.7442e-02],\n",
            "        [-1.3447e-02, -7.8576e-03,  1.1100e-02,  6.2944e-02,  1.0007e-01,\n",
            "          2.9531e-02,  9.8009e-02,  1.1164e-01, -1.1937e-01,  4.2610e-02,\n",
            "          5.5247e-02, -8.5944e-02, -3.1576e-02, -9.8741e-02, -2.2063e-02,\n",
            "         -6.2965e-02,  1.0244e-01,  5.7678e-02, -5.7253e-02,  6.1077e-02,\n",
            "          7.7593e-03, -1.2090e-01,  1.5972e-02, -3.7218e-02,  3.5571e-02,\n",
            "          1.9490e-02,  7.4432e-02,  1.6212e-02, -3.9643e-02, -1.1614e-01,\n",
            "         -6.5217e-02, -8.2196e-02, -4.7154e-02, -1.1854e-01, -1.5093e-02,\n",
            "         -1.1586e-01,  1.0680e-01,  2.5450e-02,  1.1221e-01,  3.0323e-02,\n",
            "          8.1109e-03, -9.7578e-02, -2.9048e-02,  7.2386e-03,  4.3806e-02,\n",
            "          8.2093e-02,  4.3497e-02,  8.4582e-02, -5.9895e-02,  2.5604e-02,\n",
            "         -1.4807e-02, -9.5463e-02,  4.4645e-03, -8.9883e-02, -6.8999e-02,\n",
            "          4.4501e-02, -4.5470e-02,  4.0978e-03, -6.1050e-02, -2.7426e-03,\n",
            "         -6.4175e-02, -2.8704e-02,  8.9661e-03,  7.9657e-02],\n",
            "        [-4.8851e-02,  7.5654e-02, -1.3970e-02,  1.0219e-01,  2.1301e-02,\n",
            "         -7.2557e-02, -6.6015e-02, -5.5027e-02,  4.0036e-02,  1.1232e-01,\n",
            "          5.0932e-02, -5.0646e-02,  5.0261e-03,  6.7477e-02, -1.1152e-01,\n",
            "          6.8823e-02,  3.4872e-02, -4.7150e-03, -9.9587e-02, -2.2807e-02,\n",
            "          4.7994e-03, -1.0990e-01, -2.4916e-02, -9.2341e-02,  2.8980e-02,\n",
            "         -1.0717e-01,  4.7297e-02,  8.7596e-02, -6.6486e-02, -7.5105e-02,\n",
            "         -8.8551e-02, -7.6766e-03,  8.3073e-02,  5.3626e-02,  8.9131e-02,\n",
            "          6.5086e-02, -9.1552e-03, -4.2444e-02, -5.5540e-02,  4.6005e-02,\n",
            "         -8.9542e-02,  2.0962e-02, -9.4481e-02, -1.4079e-02, -1.0906e-01,\n",
            "          9.9984e-04,  6.3327e-02,  2.7889e-02, -6.8758e-02, -7.1932e-02,\n",
            "          9.1510e-02, -8.4289e-02, -1.0104e-01, -1.1953e-01,  9.3039e-02,\n",
            "          7.7148e-03,  1.5020e-02, -5.6714e-02,  8.5667e-02, -2.4684e-02,\n",
            "         -2.9197e-03, -8.1952e-02, -5.2021e-02, -9.2035e-02],\n",
            "        [-6.0809e-02,  1.1139e-01,  4.0600e-02, -4.7570e-02,  1.0520e-01,\n",
            "         -4.6676e-02,  3.4012e-02,  9.3933e-02,  5.3803e-02, -5.3443e-02,\n",
            "         -1.0863e-01,  8.6826e-02,  8.8265e-02,  6.0337e-02, -6.0826e-02,\n",
            "          7.6330e-02, -9.4802e-02,  8.9443e-02,  1.5927e-02, -3.5941e-02,\n",
            "          2.8379e-02,  1.0907e-01, -5.2875e-03, -8.1354e-03, -4.7225e-02,\n",
            "          2.4567e-02,  1.1522e-01,  1.0002e-01, -4.0246e-02, -1.0207e-02,\n",
            "         -6.7888e-02, -1.0000e-01,  4.3312e-02,  9.1698e-02, -1.0705e-01,\n",
            "          7.0153e-02,  5.2095e-02,  2.0406e-02, -1.2247e-01, -1.0232e-01,\n",
            "          1.0560e-01, -7.3400e-02, -7.7217e-02, -3.3357e-02, -7.1977e-02,\n",
            "         -7.0002e-02, -3.7136e-02, -9.6553e-03, -3.3483e-02,  1.1512e-01,\n",
            "         -7.1550e-02, -8.8732e-02,  5.7070e-02,  1.1747e-01, -2.8269e-02,\n",
            "          1.1627e-02, -9.9662e-02,  7.7118e-02,  1.2558e-02,  1.0402e-01,\n",
            "         -8.9181e-02,  1.0060e-01, -1.2081e-01, -1.1759e-01],\n",
            "        [-6.5359e-02, -7.6783e-02,  4.3868e-02,  1.1528e-02, -1.0509e-01,\n",
            "         -6.3134e-02,  4.6945e-02,  8.4098e-02,  1.0741e-01,  1.0048e-01,\n",
            "          4.2427e-02,  6.9148e-02, -4.8341e-02,  7.4919e-02,  8.1625e-02,\n",
            "         -2.5497e-02,  1.8119e-02, -1.1784e-01,  9.0766e-02, -1.2045e-02,\n",
            "         -1.0661e-01,  7.1623e-02,  5.2837e-02, -7.8707e-02, -1.7530e-02,\n",
            "          9.8751e-02, -5.5501e-02,  2.8099e-02,  1.7898e-02, -9.8881e-02,\n",
            "          2.6716e-02, -5.8873e-02,  7.5689e-02, -5.5816e-02, -7.5582e-03,\n",
            "          1.1458e-01, -3.6555e-02,  8.8240e-02, -6.0666e-02, -6.5777e-02,\n",
            "          3.8582e-02, -1.2195e-01, -6.8963e-02,  9.2077e-02, -4.4175e-02,\n",
            "          3.0049e-02,  3.6670e-02, -1.0693e-01, -4.2976e-02, -8.2085e-02,\n",
            "         -1.1828e-01, -1.0283e-02,  3.5713e-02,  4.8593e-02, -1.0387e-01,\n",
            "          4.6941e-02,  1.4907e-02, -2.0234e-02,  8.8479e-03,  8.2318e-02,\n",
            "         -5.8523e-02, -1.2389e-01,  5.1500e-02,  1.2408e-01],\n",
            "        [-7.2356e-02, -6.1829e-02, -7.1917e-02, -9.1037e-02, -4.5577e-02,\n",
            "          7.3358e-02, -1.0780e-01,  1.0122e-01, -1.0386e-01,  3.3816e-02,\n",
            "          5.7832e-02, -2.2380e-02, -7.3459e-02, -1.1258e-01, -1.1102e-01,\n",
            "          9.6248e-02, -1.8428e-02, -5.0377e-02,  1.2437e-01,  1.8461e-02,\n",
            "          2.0103e-02,  2.8859e-02, -7.1016e-02, -2.0843e-02,  9.6889e-02,\n",
            "          7.1125e-02,  4.5673e-02, -6.8591e-02, -1.2741e-02,  5.5474e-02,\n",
            "          8.4850e-02, -8.8740e-02, -2.9636e-03,  1.1743e-01,  5.8419e-02,\n",
            "          1.0703e-01, -1.0644e-01, -6.3451e-02,  3.8023e-02, -9.0054e-02,\n",
            "         -1.6651e-02,  1.8226e-02,  2.2227e-02, -8.8957e-03,  9.9901e-02,\n",
            "         -4.8602e-03,  6.7401e-02, -1.0568e-01,  4.0040e-02,  2.9189e-02,\n",
            "         -2.7496e-02,  7.6002e-03, -1.1858e-02, -7.6634e-02,  8.0471e-02,\n",
            "         -7.0586e-03,  3.1381e-02,  9.2120e-02, -1.1553e-02,  8.6403e-02,\n",
            "          8.8227e-02, -6.3610e-02, -5.3432e-02,  6.9708e-02],\n",
            "        [-8.4946e-02,  8.6359e-02,  1.1957e-01, -4.3786e-02, -4.3922e-02,\n",
            "          2.3067e-02, -6.1465e-02, -5.6269e-02, -7.0088e-03,  2.3308e-02,\n",
            "         -1.5533e-02, -4.3382e-02,  7.5933e-02, -4.2437e-02,  8.1423e-02,\n",
            "          7.9284e-02, -3.9840e-03, -9.6882e-02,  6.3666e-02, -3.7914e-02,\n",
            "          2.3098e-02,  1.0414e-01, -2.4156e-02,  6.9440e-02,  8.1700e-02,\n",
            "          9.4583e-02,  3.9447e-02,  1.6515e-02, -9.5482e-03,  9.6213e-02,\n",
            "         -7.9347e-02, -5.8641e-02,  5.1230e-02, -1.0842e-02, -8.7477e-02,\n",
            "         -2.1937e-02, -5.8659e-02,  6.2138e-02,  1.1027e-01,  8.4062e-02,\n",
            "         -6.8705e-02,  1.5725e-02,  1.0274e-01,  1.2104e-01, -1.1408e-01,\n",
            "          5.5006e-02,  6.7451e-02,  5.4776e-02, -3.3359e-02, -8.5550e-02,\n",
            "         -1.1337e-01, -6.7574e-02, -1.1839e-01, -7.4110e-03, -4.3830e-02,\n",
            "          5.0627e-02,  1.2202e-01, -9.9533e-03, -1.0149e-01,  6.2598e-02,\n",
            "         -4.2140e-02,  9.9166e-02,  9.2266e-02, -1.9732e-03],\n",
            "        [ 1.0105e-01, -1.8103e-02, -7.6992e-02,  5.5982e-02,  3.0865e-02,\n",
            "          1.2405e-01,  9.1889e-02, -3.0146e-02, -9.2671e-02,  5.3965e-02,\n",
            "         -6.8592e-02, -1.0776e-01,  5.4518e-02,  8.3327e-02,  2.1959e-02,\n",
            "         -6.4492e-02, -6.9369e-02, -2.2197e-02, -2.3503e-02, -8.0484e-02,\n",
            "          5.5658e-02, -4.4710e-02, -8.7596e-02,  2.3246e-02, -9.0641e-03,\n",
            "         -1.0055e-01, -2.5463e-03,  4.8679e-02, -5.4767e-02,  9.2572e-02,\n",
            "          8.2081e-02,  2.9347e-02, -2.1293e-02,  8.1051e-02, -2.3319e-02,\n",
            "          6.0066e-02,  9.0596e-02, -1.0205e-01, -6.3254e-02,  6.2603e-02,\n",
            "         -7.0569e-02, -5.3463e-02,  3.6116e-02,  2.9015e-02,  1.1115e-01,\n",
            "         -1.0102e-01, -8.6102e-03, -7.7031e-02,  8.3625e-02,  1.2446e-01,\n",
            "         -1.1150e-01, -5.0594e-02, -7.8977e-02, -1.1906e-01,  7.0672e-02,\n",
            "          1.9671e-02, -1.1982e-01, -6.9265e-02,  7.5291e-02, -1.1150e-01,\n",
            "         -2.3370e-02,  7.4516e-02,  1.0915e-01, -1.0151e-01]])), ('fc2.bias', tensor([ 0.0470,  0.0030,  0.0664, -0.0259,  0.0584,  0.0128,  0.0759,  0.0185,\n",
            "         0.0326, -0.0149]))])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-222e5e646573>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m# Paso 2: Retropropagar la pérdida con respecto a los parámetros de la red mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0moptimizer_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Limpiar los gradientes acumulados de la máscara\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0moutputs_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Obtener la salida de la máscara (ficticio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0;31m# Calcular el loss de mask borrando el loss real de mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-c3ae4789373a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    165\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.item()"
      ],
      "metadata": {
        "id": "uUYghMCiP_tq",
        "outputId": "97c74005-1241-4170-aa4d-5f34c25b3a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3116891384124756"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "id": "Qa1vyj-GQB9K",
        "outputId": "63740665-f01a-43d9-e9cb-0559a7d79db2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3117, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_mask.item()"
      ],
      "metadata": {
        "id": "0UkKegJzQTS6",
        "outputId": "03909724-a433-4254-ba84-1d234d5f9550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}