{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_epochs = 30\n",
        "early_stop_patience = 5\n",
        "data_dir = '../data/MNIST'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.1307, 0.1307, 0.1307],\n",
        "                         std=[0.3081, 0.3081, 0.3081])\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root= data_dir, train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root= data_dir, train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.alexnet(pretrained=False)\n",
        "model.classifier[6] = nn.Linear(4096, 10)\n",
        "model.to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_losses, test_losses, train_accs, test_accs = [], [], [], []\n",
        "best_loss = float('inf')\n",
        "patience = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total, correct, train_loss = 0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    train_losses.append(train_loss / len(train_loader))\n",
        "    train_accs.append(acc)\n",
        "\n",
        "    model.eval()\n",
        "    total, correct, val_loss = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            val_loss += criterion(out, y).item()\n",
        "            correct += (out.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    val_acc = correct / total\n",
        "    test_losses.append(val_loss / len(test_loader))\n",
        "    test_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_losses[-1]:.4f} | Test Loss: {test_losses[-1]:.4f} | Train Acc: {acc:.4f} | Test Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if test_losses[-1] < best_loss:\n",
        "        best_loss = test_losses[-1]\n",
        "        best_model = model.state_dict()\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= early_stop_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(best_model)\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        correct += (model(x).argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(f\"\\nâœ… Final CIFAR10 Test Accuracy: {correct / total:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        outputs = model(x)\n",
        "        preds = outputs.argmax(1).cpu()\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(y.numpy())\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=test_dataset.classes if hasattr(test_dataset, 'classes') else [str(i) for i in range(10)]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(test_losses, label=\"Test Loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label=\"Train Accuracy\")\n",
        "plt.plot(test_accs, label=\"Test Accuracy\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOQv2Mo+cIH7LuOhU2O8Ewa",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
